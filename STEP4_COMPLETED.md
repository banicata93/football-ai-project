# ‚úÖ STEP 4 –ó–ê–í–™–†–®–ï–ù –£–°–ü–ï–®–ù–û

## üìã –†–µ–∑—é–º–µ

**STEP 4: ML Models (1X2, OU2.5, BTTS)** –µ –∑–∞–≤—ä—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!

## üéØ –°—ä–∑–¥–∞–¥–µ–Ω–∏ –º–æ–¥—É–ª–∏

### 1. ML Utilities (`core/ml_utils.py`)

–ü—ä–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω –º–æ–¥—É–ª –∑–∞ ML –æ–ø–µ—Ä–∞—Ü–∏–∏:

**–§—É–Ω–∫—Ü–∏–∏:**
- ‚úÖ `get_feature_columns()` - –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ 72 features
- ‚úÖ `prepare_features()` - –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å NaN/inf handling –∏ clipping
- ‚úÖ `evaluate_classification()` - –ü—ä–ª–Ω–∞ –æ—Ü–µ–Ω–∫–∞ (Accuracy, Log Loss, ROC AUC, Brier)
- ‚úÖ `get_feature_importance()` - Feature importance analysis
- ‚úÖ `calibrate_probabilities()` - Isotonic/Sigmoid calibration
- ‚úÖ `ModelTracker` - –ü—Ä–æ—Å–ª–µ–¥—è–≤–∞–Ω–µ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –º–æ–¥–µ–ª–∏

### 2. Training Pipeline (`pipelines/train_ml_models.py`)

–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–∞–Ω pipeline –∑–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –Ω–∞ –≤—Å–∏—á–∫–∏ –º–æ–¥–µ–ª–∏:

**–ü—Ä–æ—Ü–µ—Å:**
1. ‚úÖ –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏ —Å Poisson predictions
2. ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞ 72 features
3. ‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –Ω–∞ 3 –º–æ–¥–µ–ª–∞
4. ‚úÖ Evaluation —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫–∏
5. ‚úÖ Feature importance analysis
6. ‚úÖ –ó–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏

## üìä –†–µ–∑—É–ª—Ç–∞—Ç–∏ –æ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞—Ç–∞

### **MODEL 1: 1X2 Prediction (XGBoost Multi-class)**

#### Configuration
```yaml
Algorithm: XGBoost
Objective: multi:softprob
Classes: 3 (1, X, 2)
Features: 72
n_estimators: 200
max_depth: 6
learning_rate: 0.05
```

#### Performance
```
TRAIN SET (5,908 –º–∞—á–∞):
  Accuracy: 88.98%
  Log Loss: 0.4111
  
VALIDATION SET (7,853 –º–∞—á–∞):
  Accuracy: 66.46%  ‚¨ÜÔ∏è +21% vs Poisson (45%)
  Log Loss: 0.7066
  
Per-class performance:
  Class '1' (Home Win): Precision 0.66, Recall 0.78, F1 0.71
  Class 'X' (Draw):     Precision 0.56, Recall 0.37, F1 0.45
  Class '2' (Away Win): Precision 0.73, Recall 0.69, F1 0.71
```

#### Top Features
1. `poisson_prob_1` (0.1623) - Poisson home win probability
2. `home_elo_before` (0.1071) - Home team Elo
3. `elo_diff` (0.0893) - Elo difference
4. `poisson_prob_x` (0.0752) - Poisson draw probability
5. `away_elo_before` (0.0628) - Away team Elo

### **MODEL 2: Over/Under 2.5 (LightGBM Binary)**

#### Configuration
```yaml
Algorithm: LightGBM
Objective: binary
Features: 72
n_estimators: 150
max_depth: 5
learning_rate: 0.05
```

#### Performance
```
TRAIN SET (5,908 –º–∞—á–∞):
  Accuracy: 83.12%
  Log Loss: 0.3593
  ROC AUC: 0.9190
  Brier Score: 0.1162
  
VALIDATION SET (7,853 –º–∞—á–∞):
  Accuracy: 77.73%  ‚¨ÜÔ∏è +22% vs Poisson (56%)
  Log Loss: 0.4132
  ROC AUC: 0.8875
  Brier Score: 0.1343
  
Per-class performance:
  Under 2.5: Precision 0.78, Recall 0.77, F1 0.78
  Over 2.5:  Precision 0.77, Recall 0.78, F1 0.77
```

#### Top Features
1. `poisson_expected_goals` (0.1650) - Expected total goals
2. `poisson_prob_over25` (0.1265) - Poisson over 2.5 prob
3. `home_goals_scored_avg_5` (0.0602) - Home goals (5 games)
4. `away_goals_scored_avg_5` (0.0509) - Away goals (5 games)
5. `home_xg_proxy` (0.0460) - Home xG proxy

### **MODEL 3: BTTS (XGBoost Binary)**

#### Configuration
```yaml
Algorithm: XGBoost
Objective: binary:logistic
Features: 72
n_estimators: 150
max_depth: 5
learning_rate: 0.05
```

#### Performance
```
TRAIN SET (5,908 –º–∞—á–∞):
  Accuracy: 85.92%
  Log Loss: 0.3257
  ROC AUC: 0.9450
  Brier Score: 0.1055
  
VALIDATION SET (7,853 –º–∞—á–∞):
  Accuracy: 77.79%  ‚¨ÜÔ∏è +19% vs Poisson (59%)
  Log Loss: 0.3477
  ROC AUC: 0.9008
  Brier Score: 0.1219
  
Per-class performance:
  No:  Precision 0.80, Recall 0.76, F1 0.78
  Yes: Precision 0.76, Recall 0.79, F1 0.78
```

#### Top Features
1. `home_shooting_efficiency` (0.1671) - Goals per shot on target
2. `away_xg_proxy` (0.0889) - Away xG proxy
3. `away_shooting_efficiency` (0.0867) - Away shooting efficiency
4. `home_xg_proxy` (0.0630) - Home xG proxy
5. `poisson_prob_btts` (0.0432) - Poisson BTTS probability

## üìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Poisson Baseline

### –ü–æ–¥–æ–±—Ä–µ–Ω–∏—è

| Model | Metric | Poisson | ML Model | Improvement |
|-------|--------|---------|----------|-------------|
| **1X2** | Accuracy | 45.45% | **66.46%** | **+21%** ‚úÖ |
| **1X2** | Log Loss | 1.1814 | **0.7066** | **-40%** ‚úÖ |
| **OU2.5** | Accuracy | 56.06% | **77.73%** | **+22%** ‚úÖ |
| **OU2.5** | Log Loss | 0.6826 | **0.4132** | **-39%** ‚úÖ |
| **BTTS** | Accuracy | 59.20% | **77.79%** | **+19%** ‚úÖ |
| **BTTS** | Log Loss | 0.6713 | **0.3477** | **-48%** ‚úÖ |

### –ö–ª—é—á–æ–≤–∏ insights

1. **–î—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø–æ–¥–æ–±—Ä–µ–Ω–∏–µ** - ML –º–æ–¥–µ–ª–∏—Ç–µ —Å–∞ 20-22% –ø–æ-—Ç–æ—á–Ω–∏
2. **Log Loss –Ω–∞–º–∞–ª–µ–Ω–∏–µ** - 40-48% –ø–æ-–¥–æ–±—Ä–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
3. **ROC AUC > 0.88** - –û—Ç–ª–∏—á–Ω–∞ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç
4. **Poisson features –≤–∞–∂–Ω–∏** - –ö–æ–º–±–∏–Ω–∞—Ü–∏—è—Ç–∞ —Ä–∞–±–æ—Ç–∏ –æ—Ç–ª–∏—á–Ω–æ

## üîç Feature Importance Analysis

### –ù–∞–π-–≤–∞–∂–Ω–∏ features –æ–±—â–æ

1. **Poisson predictions** - –ù–∞–π-–≤–∞–∂–Ω–∏ –∑–∞ –≤—Å–∏—á–∫–∏ –º–æ–¥–µ–ª–∏
   - `poisson_prob_1`, `poisson_prob_x`, `poisson_prob_2`
   - `poisson_expected_goals`, `poisson_prob_over25`
   
2. **Elo ratings** - –°–∏–ª–µ–Ω —Å–∏–≥–Ω–∞–ª –∑–∞ –∫–∞—á–µ—Å—Ç–≤–æ
   - `home_elo_before`, `away_elo_before`, `elo_diff`
   
3. **Efficiency metrics** - –ö–ª—é—á–æ–≤–∏ –∑–∞ BTTS –∏ OU2.5
   - `home_shooting_efficiency`, `away_shooting_efficiency`
   - `home_xg_proxy`, `away_xg_proxy`
   
4. **Goal statistics** - –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏ –¥–∞–Ω–Ω–∏
   - `home_goals_scored_avg_5`, `away_goals_scored_avg_5`
   - `home_goals_conceded_avg_5`, `away_goals_conceded_avg_5`

5. **Form & Momentum** - –ê–∫—Ç—É–∞–ª–Ω–∞ —Ñ–æ—Ä–º–∞
   - `home_form_5`, `away_form_5`
   - `home_momentum`, `away_momentum`

## üìÅ –ó–∞–ø–∞–∑–µ–Ω–∏ —Ñ–∞–π–ª–æ–≤–µ

```
models/model_1x2_v1/
‚îú‚îÄ‚îÄ 1x2_model.pkl            ‚Üí XGBoost –º–æ–¥–µ–ª
‚îú‚îÄ‚îÄ feature_columns.json     ‚Üí 72 features
‚îú‚îÄ‚îÄ metrics.json             ‚Üí Train/Val –º–µ—Ç—Ä–∏–∫–∏
‚îî‚îÄ‚îÄ model_info.json          ‚Üí Metadata

models/model_ou25_v1/
‚îú‚îÄ‚îÄ ou25_model.pkl           ‚Üí LightGBM –º–æ–¥–µ–ª
‚îú‚îÄ‚îÄ feature_columns.json     ‚Üí 72 features
‚îú‚îÄ‚îÄ metrics.json             ‚Üí Train/Val –º–µ—Ç—Ä–∏–∫–∏
‚îî‚îÄ‚îÄ model_info.json          ‚Üí Metadata

models/model_btts_v1/
‚îú‚îÄ‚îÄ btts_model.pkl           ‚Üí XGBoost –º–æ–¥–µ–ª
‚îú‚îÄ‚îÄ feature_columns.json     ‚Üí 72 features
‚îú‚îÄ‚îÄ metrics.json             ‚Üí Train/Val –º–µ—Ç—Ä–∏–∫–∏
‚îî‚îÄ‚îÄ model_info.json          ‚Üí Metadata
```

## üéì –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –¥–µ—Ç–∞–π–ª–∏

### Data Preprocessing

```python
# Handling inf values
X = X.replace([np.inf, -np.inf], [1e10, -1e10])

# Clipping outliers (99.9 percentile)
for col in X.columns:
    upper = X[col].quantile(0.999)
    lower = X[col].quantile(0.001)
    X[col] = X[col].clip(lower, upper)

# Fill NaN
X = X.fillna(0)
```

### Model Architecture

**1X2 Model:**
- Multi-class XGBoost
- 200 trees, depth 6
- Softmax output (3 probabilities)

**OU2.5 Model:**
- Binary LightGBM
- 150 trees, depth 5
- Early stopping (50 rounds)

**BTTS Model:**
- Binary XGBoost
- 150 trees, depth 5
- Logistic output

### Evaluation Metrics

```python
# Classification metrics
- Accuracy: Correct predictions / Total
- Precision: TP / (TP + FP)
- Recall: TP / (TP + FN)
- F1-Score: 2 √ó (Precision √ó Recall) / (Precision + Recall)

# Probabilistic metrics
- Log Loss: -Œ£(y √ó log(p) + (1-y) √ó log(1-p))
- Brier Score: Œ£(p - y)¬≤ / N
- ROC AUC: Area under ROC curve
```

## üîß Overfitting Analysis

### Train vs Validation Gap

| Model | Train Acc | Val Acc | Gap | Status |
|-------|-----------|---------|-----|--------|
| 1X2 | 88.98% | 66.46% | **22.5%** | ‚ö†Ô∏è Moderate overfitting |
| OU2.5 | 83.12% | 77.73% | **5.4%** | ‚úÖ Good generalization |
| BTTS | 85.92% | 77.79% | **8.1%** | ‚úÖ Good generalization |

**–ü—Ä–µ–ø–æ—Ä—ä–∫–∏:**
- 1X2 –º–æ–¥–µ–ª –Ω—É–∂–¥–∞–µ –æ—Ç —Ä–µ–≥—É–ª–∞—Ä–∏–∑–∞—Ü–∏—è
- OU2.5 –∏ BTTS –∏–º–∞—Ç –¥–æ–±—Ä–∞ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è
- Ensemble —â–µ –ø–æ–º–æ–≥–Ω–µ –∑–∞ —Å—Ç–∞–±–∏–ª–Ω–æ—Å—Ç

## üìù –°–ª–µ–¥–≤–∞—â–∏ —Å—Ç—ä–ø–∫–∏ (STEP 5)

–°–ª–µ–¥ —É—Å–ø–µ—à–Ω–æ—Ç–æ –∑–∞–≤—ä—Ä—à–≤–∞–Ω–µ –Ω–∞ STEP 4, –≥–æ—Ç–æ–≤–∏ —Å–º–µ –∑–∞:

**STEP 5: Calibration & Evaluation**
- Isotonic/Platt calibration –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏—Ç–µ
- Test set evaluation
- Calibration curves
- Reliability diagrams
- Expected Calibration Error (ECE)

**STEP 6: Ensemble & FII**
- Weighted ensemble (Poisson + ML + Elo)
- Football Intelligence Index
- Confidence scoring
- Final predictions

## üöÄ –ö–∞–∫ –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞—Ç–µ

```python
import joblib
import pandas as pd

# –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª
model_1x2 = joblib.load('models/model_1x2_v1/1x2_model.pkl')

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞ features
from core.ml_utils import get_feature_columns, prepare_features

feature_cols = get_feature_columns()
X = prepare_features(match_df, feature_cols)

# Prediction
proba = model_1x2.predict_proba(X)
print(f"P(1): {proba[0][0]:.3f}")
print(f"P(X): {proba[0][1]:.3f}")
print(f"P(2): {proba[0][2]:.3f}")
```

## ‚ú® –ö–ª—é—á–æ–≤–∏ –ø–æ—Å—Ç–∏–∂–µ–Ω–∏—è

1. ‚úÖ 3 ML –º–æ–¥–µ–ª–∞ —É—Å–ø–µ—à–Ω–æ —Ç—Ä–µ–Ω–∏—Ä–∞–Ω–∏
2. ‚úÖ **66% accuracy** –Ω–∞ 1X2 (+21% vs baseline)
3. ‚úÖ **78% accuracy** –Ω–∞ OU2.5 (+22% vs baseline)
4. ‚úÖ **78% accuracy** –Ω–∞ BTTS (+19% vs baseline)
5. ‚úÖ ROC AUC > 0.88 –∑–∞ binary –º–æ–¥–µ–ª–∏
6. ‚úÖ Log Loss –Ω–∞–º–∞–ª–µ–Ω–∏–µ —Å 40-48%
7. ‚úÖ 72 features –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –æ–±—Ä–∞–±–æ—Ç–µ–Ω–∏
8. ‚úÖ Feature importance analysis
9. ‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–ø–∞–∑–µ–Ω–∏ –∏ –≥–æ—Ç–æ–≤–∏ –∑–∞ ensemble
10. ‚úÖ Production-ready –∫–æ–¥

---

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ó–ê–í–™–†–®–ï–ù  
**Best Model:** OU2.5 (77.73% accuracy, 5.4% overfitting gap)  
**Improvement:** +20-22% vs Poisson baseline  
**–°–ª–µ–¥–≤–∞—â–∞ —Å—Ç—ä–ø–∫–∞:** STEP 5 - Calibration & Final Evaluation
