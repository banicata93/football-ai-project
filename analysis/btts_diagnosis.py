#!/usr/bin/env python3
"""
BTTS Model Comprehensive Diagnosis
–ê–Ω–∞–ª–∏–∑–∏—Ä–∞ —Ç–µ–∫—É—â–∏—è BTTS –º–æ–¥–µ–ª –∑–∞ accuracy, calibration, bias –∏ league performance
"""

import sys
import os
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
import joblib
import json
from datetime import datetime
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    brier_score_loss, log_loss, classification_report, confusion_matrix
)
from sklearn.calibration import calibration_curve

from core.utils import setup_logging, save_json
from core.ml_utils import prepare_features, get_feature_columns


class BTTSDiagnostics:
    """Comprehensive BTTS model diagnostics"""
    
    def __init__(self):
        self.logger = setup_logging()
        self.model = None
        self.calibrator = None
        self.feature_cols = None
        
    def load_btts_model(self):
        """–ó–∞—Ä–µ–∂–¥–∞ BTTS –º–æ–¥–µ–ª–∞ –∏ –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä–∞"""
        try:
            # –ó–∞—Ä–µ–∂–¥–∞ –æ—Å–Ω–æ–≤–Ω–∏—è –º–æ–¥–µ–ª
            model_path = 'models/model_btts_v1/btts_model.pkl'
            if os.path.exists(model_path):
                self.model = joblib.load(model_path)
                self.logger.info(f"‚úì BTTS –º–æ–¥–µ–ª –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç {model_path}")
            else:
                self.logger.error(f"‚ùå BTTS –º–æ–¥–µ–ª –Ω–µ –µ –Ω–∞–º–µ—Ä–µ–Ω: {model_path}")
                return False
            
            # –ó–∞—Ä–µ–∂–¥–∞ feature columns
            feature_path = 'models/model_btts_v1/feature_columns.json'
            if os.path.exists(feature_path):
                with open(feature_path, 'r') as f:
                    feature_data = json.load(f)
                    # –ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ –µ nested —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
                    if isinstance(feature_data, dict) and 'features' in feature_data:
                        self.feature_cols = feature_data['features']
                    else:
                        self.feature_cols = feature_data
                self.logger.info(f"‚úì Feature columns –∑–∞—Ä–µ–¥–µ–Ω–∏: {len(self.feature_cols)} features")
            else:
                self.feature_cols = get_feature_columns()
                self.logger.warning("‚ö†Ô∏è –ò–∑–ø–æ–ª–∑–≤–∞–º default feature columns")
            
            # –ó–∞—Ä–µ–∂–¥–∞ –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä –∞–∫–æ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞
            calibrator_path = 'models/model_btts_v1/calibrator.pkl'
            if os.path.exists(calibrator_path):
                self.calibrator = joblib.load(calibrator_path)
                self.logger.info("‚úì BTTS –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä –∑–∞—Ä–µ–¥–µ–Ω")
            else:
                self.logger.warning("‚ö†Ô∏è BTTS –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä –Ω–µ –µ –Ω–∞–º–µ—Ä–µ–Ω")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ BTTS –º–æ–¥–µ–ª: {e}")
            return False
    
    def calculate_ece(self, y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10) -> float:
        """
        –ò–∑—á–∏—Å–ª—è–≤–∞ Expected Calibration Error (ECE)
        
        Args:
            y_true: –ò—Å—Ç–∏–Ω—Å–∫–∏ labels
            y_prob: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            n_bins: –ë—Ä–æ–π bins –∑–∞ –∫–∞–ª–∏–±—Ä–∞—Ü–∏—è
            
        Returns:
            ECE —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        ece = 0
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            # –ù–∞–º–∏—Ä–∞ samples –≤ —Ç–æ–∑–∏ bin
            in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)
            prop_in_bin = in_bin.mean()
            
            if prop_in_bin > 0:
                accuracy_in_bin = y_true[in_bin].mean()
                avg_confidence_in_bin = y_prob[in_bin].mean()
                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
        
        return ece
    
    def analyze_calibration_bins(self, y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–∞ –∫–∞–ª–∏–±—Ä–∞—Ü–∏—è—Ç–∞ –ø–æ bins
        
        Args:
            y_true: –ò—Å—Ç–∏–Ω—Å–∫–∏ labels
            y_prob: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            n_bins: –ë—Ä–æ–π bins
            
        Returns:
            Dictionary —Å –∞–Ω–∞–ª–∏–∑ –ø–æ bins
        """
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        calibration_data = []
        
        for i, (bin_lower, bin_upper) in enumerate(zip(bin_lowers, bin_uppers)):
            in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)
            
            if in_bin.sum() > 0:
                bin_data = {
                    'bin_id': i,
                    'bin_range': f'[{bin_lower:.1f}-{bin_upper:.1f}]',
                    'count': int(in_bin.sum()),
                    'avg_predicted_prob': float(y_prob[in_bin].mean()),
                    'actual_positive_rate': float(y_true[in_bin].mean()),
                    'calibration_error': float(abs(y_prob[in_bin].mean() - y_true[in_bin].mean()))
                }
                calibration_data.append(bin_data)
        
        return {
            'bins': calibration_data,
            'ece': self.calculate_ece(y_true, y_prob, n_bins),
            'total_samples': len(y_true)
        }
    
    def analyze_league_performance(self, df: pd.DataFrame, y_true: np.ndarray, y_prob: np.ndarray) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–∞ BTTS performance –ø–æ –ª–∏–≥–∏
        
        Args:
            df: Dataset —Å league –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            y_true: –ò—Å—Ç–∏–Ω—Å–∫–∏ BTTS labels
            y_prob: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏ BTTS –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            
        Returns:
            Dictionary —Å league –∞–Ω–∞–ª–∏–∑
        """
        league_analysis = {}
        
        if 'league' not in df.columns:
            self.logger.warning("‚ö†Ô∏è –ù—è–º–∞ league –∫–æ–ª–æ–Ω–∞ –∑–∞ –∞–Ω–∞–ª–∏–∑")
            return {}
        
        for league in df['league'].unique():
            if pd.isna(league):
                continue
                
            league_mask = df['league'] == league
            league_y_true = y_true[league_mask]
            league_y_prob = y_prob[league_mask]
            
            if len(league_y_true) < 10:  # –ú–∏–Ω–∏–º—É–º samples
                continue
            
            # –û—Å–Ω–æ–≤–Ω–∏ –º–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(league_y_true, (league_y_prob > 0.5).astype(int))
            brier = brier_score_loss(league_y_true, league_y_prob)
            ece = self.calculate_ece(league_y_true, league_y_prob)
            
            # BTTS base rate –≤ –ª–∏–≥–∞—Ç–∞
            btts_rate = league_y_true.mean()
            
            # Bias –∞–Ω–∞–ª–∏–∑
            avg_predicted_prob = league_y_prob.mean()
            bias = avg_predicted_prob - btts_rate
            
            league_analysis[league] = {
                'matches': int(len(league_y_true)),
                'btts_base_rate': float(btts_rate),
                'avg_predicted_prob': float(avg_predicted_prob),
                'bias': float(bias),
                'accuracy': float(accuracy),
                'brier_score': float(brier),
                'ece': float(ece),
                'btts_yes_count': int(league_y_true.sum()),
                'btts_no_count': int(len(league_y_true) - league_y_true.sum())
            }
        
        return league_analysis
    
    def comprehensive_evaluation(self, df: pd.DataFrame) -> Dict:
        """
        –ü—ä–ª–Ω–∞ –æ—Ü–µ–Ω–∫–∞ –Ω–∞ BTTS –º–æ–¥–µ–ª–∞
        
        Args:
            df: Test/validation dataset
            
        Returns:
            Comprehensive evaluation results
        """
        if self.model is None:
            raise ValueError("BTTS –º–æ–¥–µ–ª –Ω–µ –µ –∑–∞—Ä–µ–¥–µ–Ω")
        
        self.logger.info("üîç –ó–∞–ø–æ—á–≤–∞–Ω–µ –Ω–∞ comprehensive BTTS evaluation...")
        
        # –ü–æ–¥–≥–æ—Ç–≤—è features
        X = prepare_features(df, self.feature_cols)
        y_true = df['btts'].values
        
        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ –º–æ–¥–µ–ª—ä—Ç –µ –≤–µ—á–µ –∫–∞–ª–∏–±—Ä–∏—Ä–∞–Ω
        from sklearn.calibration import CalibratedClassifierCV
        
        if isinstance(self.model, CalibratedClassifierCV):
            # –ú–æ–¥–µ–ª—ä—Ç –µ –≤–µ—á–µ –∫–∞–ª–∏–±—Ä–∏—Ä–∞–Ω
            y_pred = self.model.predict(X)
            y_prob_calibrated = self.model.predict_proba(X)[:, 1]
            
            # –ó–∞ raw predictions –∏–∑–ø–æ–ª–∑–≤–∞–º–µ base estimator
            base_estimator = self.model.estimator
            y_prob_raw = base_estimator.predict_proba(X)[:, 1]
        else:
            # –û–±–∏–∫–Ω–æ–≤–µ–Ω –º–æ–¥–µ–ª
            y_pred = self.model.predict(X)
            y_prob_raw = self.model.predict_proba(X)[:, 1]
            
            # –ö–∞–ª–∏–±—Ä–∏—Ä–∞–Ω–∏ predictions –∞–∫–æ –∏–º–∞ –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä
            if self.calibrator is not None:
                y_prob_calibrated = self.calibrator.predict_proba(X)[:, 1]
            else:
                y_prob_calibrated = y_prob_raw
        
        # –û—Å–Ω–æ–≤–Ω–∏ –º–µ—Ç—Ä–∏–∫–∏
        basic_metrics = {
            'accuracy': float(accuracy_score(y_true, y_pred)),
            'precision_yes': float(precision_score(y_true, y_pred, pos_label=1)),
            'precision_no': float(precision_score(y_true, y_pred, pos_label=0)),
            'recall_yes': float(recall_score(y_true, y_pred, pos_label=1)),
            'recall_no': float(recall_score(y_true, y_pred, pos_label=0)),
            'f1_yes': float(f1_score(y_true, y_pred, pos_label=1)),
            'f1_no': float(f1_score(y_true, y_pred, pos_label=0)),
            'f1_macro': float(f1_score(y_true, y_pred, average='macro')),
            'brier_score_raw': float(brier_score_loss(y_true, y_prob_raw)),
            'brier_score_calibrated': float(brier_score_loss(y_true, y_prob_calibrated)),
            'log_loss_raw': float(log_loss(y_true, y_prob_raw)),
            'log_loss_calibrated': float(log_loss(y_true, y_prob_calibrated))
        }
        
        # –ö–∞–ª–∏–±—Ä–∞—Ü–∏–æ–Ω–µ–Ω –∞–Ω–∞–ª–∏–∑
        calibration_raw = self.analyze_calibration_bins(y_true, y_prob_raw)
        calibration_calibrated = self.analyze_calibration_bins(y_true, y_prob_calibrated)
        
        # League –∞–Ω–∞–ª–∏–∑
        league_analysis = self.analyze_league_performance(df, y_true, y_prob_calibrated)
        
        # Bias –∞–Ω–∞–ª–∏–∑
        btts_base_rate = y_true.mean()
        avg_predicted_prob = y_prob_calibrated.mean()
        overall_bias = avg_predicted_prob - btts_base_rate
        
        # Threshold –∞–Ω–∞–ª–∏–∑
        threshold_analysis = {}
        for threshold in [0.45, 0.5, 0.55, 0.6]:
            y_pred_thresh = (y_prob_calibrated > threshold).astype(int)
            threshold_analysis[f'threshold_{threshold}'] = {
                'accuracy': float(accuracy_score(y_true, y_pred_thresh)),
                'f1_macro': float(f1_score(y_true, y_pred_thresh, average='macro')),
                'precision_yes': float(precision_score(y_true, y_pred_thresh, pos_label=1)),
                'recall_yes': float(recall_score(y_true, y_pred_thresh, pos_label=1))
            }
        
        # Confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'dataset_info': {
                'total_matches': int(len(df)),
                'btts_yes': int(y_true.sum()),
                'btts_no': int(len(y_true) - y_true.sum()),
                'btts_base_rate': float(btts_base_rate)
            },
            'basic_metrics': basic_metrics,
            'bias_analysis': {
                'btts_base_rate': float(btts_base_rate),
                'avg_predicted_prob': float(avg_predicted_prob),
                'overall_bias': float(overall_bias),
                'bias_interpretation': 'Overconfident' if overall_bias > 0.05 else 'Underconfident' if overall_bias < -0.05 else 'Well calibrated'
            },
            'calibration_analysis': {
                'raw_model': calibration_raw,
                'calibrated_model': calibration_calibrated,
                'calibration_improvement': float(calibration_raw['ece'] - calibration_calibrated['ece'])
            },
            'league_analysis': league_analysis,
            'threshold_analysis': threshold_analysis,
            'confusion_matrix': {
                'tn': int(cm[0, 0]),
                'fp': int(cm[0, 1]),
                'fn': int(cm[1, 0]),
                'tp': int(cm[1, 1])
            }
        }
        
        return results
    
    def generate_report(self, results: Dict, output_path: str = 'reports/btts_diagnosis_report.json'):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–∞ –¥–µ—Ç–∞–π–ª–µ–Ω –æ—Ç—á–µ—Ç
        
        Args:
            results: –†–µ–∑—É–ª—Ç–∞—Ç–∏ –æ—Ç evaluation
            output_path: –ü—ä—Ç –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ –æ—Ç—á–µ—Ç–∞
        """
        # –ó–∞–ø–∞–∑–≤–∞ JSON –æ—Ç—á–µ—Ç–∞
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        save_json(results, output_path)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä–∞ markdown summary
        markdown_path = output_path.replace('.json', '.md')
        
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write("# BTTS Model Diagnosis Report\n\n")
            f.write(f"**Generated:** {results['timestamp']}\n\n")
            
            # Dataset info
            dataset = results['dataset_info']
            f.write("## Dataset Overview\n")
            f.write(f"- **Total matches:** {dataset['total_matches']:,}\n")
            f.write(f"- **BTTS Yes:** {dataset['btts_yes']:,} ({dataset['btts_base_rate']:.1%})\n")
            f.write(f"- **BTTS No:** {dataset['btts_no']:,} ({1-dataset['btts_base_rate']:.1%})\n\n")
            
            # Basic metrics
            metrics = results['basic_metrics']
            f.write("## Model Performance\n")
            f.write(f"- **Accuracy:** {metrics['accuracy']:.3f}\n")
            f.write(f"- **F1 Score (Macro):** {metrics['f1_macro']:.3f}\n")
            f.write(f"- **Precision (Yes):** {metrics['precision_yes']:.3f}\n")
            f.write(f"- **Recall (Yes):** {metrics['recall_yes']:.3f}\n")
            f.write(f"- **Brier Score (Raw):** {metrics['brier_score_raw']:.4f}\n")
            f.write(f"- **Brier Score (Calibrated):** {metrics['brier_score_calibrated']:.4f}\n\n")
            
            # Bias analysis
            bias = results['bias_analysis']
            f.write("## Bias Analysis\n")
            f.write(f"- **Base Rate:** {bias['btts_base_rate']:.1%}\n")
            f.write(f"- **Avg Predicted:** {bias['avg_predicted_prob']:.1%}\n")
            f.write(f"- **Bias:** {bias['overall_bias']:+.1%} ({bias['bias_interpretation']})\n\n")
            
            # Calibration
            cal = results['calibration_analysis']
            f.write("## Calibration Analysis\n")
            f.write(f"- **ECE (Raw):** {cal['raw_model']['ece']:.4f}\n")
            f.write(f"- **ECE (Calibrated):** {cal['calibrated_model']['ece']:.4f}\n")
            f.write(f"- **Improvement:** {cal['calibration_improvement']:+.4f}\n\n")
            
            # Top problematic leagues
            leagues = results['league_analysis']
            if leagues:
                f.write("## League Analysis (Top Issues)\n")
                # –°–æ—Ä—Ç–∏—Ä–∞ –ø–æ bias
                sorted_leagues = sorted(leagues.items(), key=lambda x: abs(x[1]['bias']), reverse=True)
                for league, data in sorted_leagues[:5]:
                    f.write(f"- **{league}:** {data['matches']} matches, ")
                    f.write(f"Base rate: {data['btts_base_rate']:.1%}, ")
                    f.write(f"Bias: {data['bias']:+.1%}, ")
                    f.write(f"ECE: {data['ece']:.3f}\n")
                f.write("\n")
            
            # Threshold analysis
            thresh = results['threshold_analysis']
            f.write("## Threshold Optimization\n")
            for threshold, data in thresh.items():
                t = threshold.replace('threshold_', '')
                f.write(f"- **{t}:** Acc: {data['accuracy']:.3f}, F1: {data['f1_macro']:.3f}\n")
        
        self.logger.info(f"‚úì –û—Ç—á–µ—Ç –∑–∞–ø–∞–∑–µ–Ω –≤ {output_path}")
        self.logger.info(f"‚úì Markdown summary –≤ {markdown_path}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞ BTTS –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞"""
    logger = setup_logging()
    
    logger.info("üîç –°–¢–ê–†–¢–ò–†–ê–ù–ï –ù–ê BTTS MODEL DIAGNOSIS")
    logger.info("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞—Ç–∞
    diagnostics = BTTSDiagnostics()
    
    # –ó–∞—Ä–µ–∂–¥–∞ –º–æ–¥–µ–ª–∞
    if not diagnostics.load_btts_model():
        logger.error("‚ùå –ù–µ –º–æ–≥–∞ –¥–∞ –∑–∞—Ä–µ–¥—è BTTS –º–æ–¥–µ–ª")
        return
    
    # –ó–∞—Ä–µ–∂–¥–∞ test features data
    test_features_path = 'data/processed/test_features.parquet'
    if not os.path.exists(test_features_path):
        logger.error(f"‚ùå Test features –Ω–µ —Å–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏: {test_features_path}")
        return
    
    logger.info(f"üìä –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ test features –æ—Ç {test_features_path}")
    test_df = pd.read_parquet(test_features_path)
    
    if 'btts' not in test_df.columns:
        logger.error("‚ùå –ù—è–º–∞ 'btts' –∫–æ–ª–æ–Ω–∞ –≤ test data")
        return
    
    logger.info(f"‚úì Test data –∑–∞—Ä–µ–¥–µ–Ω: {len(test_df)} matches")
    
    # –ò–∑–ø—ä–ª–Ω—è–≤–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞—Ç–∞
    results = diagnostics.comprehensive_evaluation(test_df)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä–∞ –æ—Ç—á–µ—Ç–∞
    diagnostics.generate_report(results)
    
    # –ü–æ–∫–∞–∑–≤–∞ key findings
    logger.info("\nüéØ KEY FINDINGS:")
    bias = results['bias_analysis']
    logger.info(f"   Bias: {bias['overall_bias']:+.1%} ({bias['bias_interpretation']})")
    
    metrics = results['basic_metrics']
    logger.info(f"   Accuracy: {metrics['accuracy']:.3f}")
    logger.info(f"   F1 (Macro): {metrics['f1_macro']:.3f}")
    logger.info(f"   Brier Score: {metrics['brier_score_calibrated']:.4f}")
    
    cal = results['calibration_analysis']
    logger.info(f"   ECE: {cal['calibrated_model']['ece']:.4f}")
    
    logger.info("\n‚úÖ BTTS –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞–≤—ä—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")


if __name__ == "__main__":
    main()
