#!/usr/bin/env python3
"""
Performance –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞ per-league –º–æ–¥–µ–ª–∏
"""

import sys
import os
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns

from core.league_utils import LEAGUE_ID_TO_SLUG, get_league_display_name
from core.utils import setup_logging


class PerLeaguePerformanceMonitor:
    """
    Performance –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞ per-league –º–æ–¥–µ–ª–∏
    """
    
    def __init__(self):
        self.logger = setup_logging()
        
    def load_training_metrics(self) -> Dict:
        """
        –ó–∞—Ä–µ–∂–¥–∞ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞—Ç–∞
        
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –ª–∏–≥–∏
        """
        try:
            with open("logs/model_reports/ou25_per_league_summary.json", 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            self.logger.warning("–ù—è–º–∞ training –º–µ—Ç—Ä–∏–∫–∏ —Ñ–∞–π–ª")
            return {}
    
    def analyze_model_performance(self, training_metrics: Dict) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–∞ performance –Ω–∞ –º–æ–¥–µ–ª–∏—Ç–µ
        
        Args:
            training_metrics: –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞—Ç–∞
        
        Returns:
            –ê–Ω–∞–ª–∏–∑ –Ω–∞ performance
        """
        self.logger.info("üìä –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ model performance...")
        
        analysis = {
            'league_rankings': {},
            'performance_tiers': {'excellent': [], 'good': [], 'average': [], 'poor': []},
            'metrics_summary': {},
            'recommendations': []
        }
        
        if not training_metrics.get('metrics_by_league'):
            return analysis
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä–∞ –≤—Å—è–∫–∞ –ª–∏–≥–∞
        league_scores = {}
        
        for league_slug, metrics in training_metrics['metrics_by_league'].items():
            if not metrics:
                continue
            league_name = get_league_display_name(league_slug)
            
            # Composite score (–ø–æ-–Ω–∏—Å–∫–∏ log_loss –∏ brier_score —Å–∞ –ø–æ-–¥–æ–±—Ä–∏)
            accuracy = metrics.get('accuracy', 0)
            log_loss = metrics.get('log_loss', 1.0)
            brier_score = metrics.get('brier_score', 0.5)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–Ω score (0-100)
            score = (accuracy * 40 +  # 40% accuracy
                    (1 - min(log_loss, 2.0) / 2.0) * 35 +  # 35% log loss (inverted)
                    (1 - min(brier_score, 1.0)) * 25) * 100  # 25% brier score (inverted)
            
            league_scores[league_slug] = {
                'name': league_name,
                'score': score,
                'accuracy': accuracy,
                'log_loss': log_loss,
                'brier_score': brier_score,
                'matches': metrics.get('matches', 0),
                'calibrated': True  # –í—Å–∏—á–∫–∏ per-league –º–æ–¥–µ–ª–∏ —Å–∞ –∫–∞–ª–∏–±—Ä–∏—Ä–∞–Ω–∏
            }
        
        # –°–æ—Ä—Ç–∏—Ä–∞ –ø–æ score
        sorted_leagues = sorted(league_scores.items(), key=lambda x: x[1]['score'], reverse=True)
        analysis['league_rankings'] = dict(sorted_leagues)
        
        # Performance tiers
        for league_slug, data in sorted_leagues:
            if data['score'] >= 80:
                analysis['performance_tiers']['excellent'].append(league_slug)
            elif data['score'] >= 70:
                analysis['performance_tiers']['good'].append(league_slug)
            elif data['score'] >= 60:
                analysis['performance_tiers']['average'].append(league_slug)
            else:
                analysis['performance_tiers']['poor'].append(league_slug)
        
        # Summary statistics
        if league_scores:
            scores = [data['score'] for data in league_scores.values()]
            accuracies = [data['accuracy'] for data in league_scores.values()]
            log_losses = [data['log_loss'] for data in league_scores.values()]
            brier_scores = [data['brier_score'] for data in league_scores.values()]
            
            analysis['metrics_summary'] = {
                'avg_score': np.mean(scores),
                'median_score': np.median(scores),
                'std_score': np.std(scores),
                'avg_accuracy': np.mean(accuracies),
                'avg_log_loss': np.mean(log_losses),
                'avg_brier_score': np.mean(brier_scores),
                'total_leagues': len(league_scores)
            }
        
        # –ü—Ä–µ–ø–æ—Ä—ä–∫–∏
        analysis['recommendations'] = self._generate_recommendations(analysis)
        
        return analysis
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–∞ –ø—Ä–µ–ø–æ—Ä—ä–∫–∏ –∑–∞ –ø–æ–¥–æ–±—Ä–µ–Ω–∏–µ
        
        Args:
            analysis: –ê–Ω–∞–ª–∏–∑ –Ω–∞ performance
        
        Returns:
            –°–ø–∏—Å—ä–∫ —Å –ø—Ä–µ–ø–æ—Ä—ä–∫–∏
        """
        recommendations = []
        
        poor_leagues = analysis['performance_tiers']['poor']
        if poor_leagues:
            recommendations.append(
                f"üî¥ {len(poor_leagues)} –ª–∏–≥–∏ —Å –ª–æ—à performance: "
                f"{', '.join([get_league_display_name(l) for l in poor_leagues])}. "
                f"–ü—Ä–µ–ø–æ—Ä—ä—á–≤–∞ —Å–µ –ø–æ–≤–µ—á–µ –¥–∞–Ω–Ω–∏ –∏–ª–∏ feature engineering."
            )
        
        excellent_leagues = analysis['performance_tiers']['excellent']
        if excellent_leagues:
            recommendations.append(
                f"üü¢ {len(excellent_leagues)} –ª–∏–≥–∏ —Å –æ—Ç–ª–∏—á–µ–Ω performance: "
                f"{', '.join([get_league_display_name(l) for l in excellent_leagues])}. "
                f"–ú–æ–¥–µ–ª–∏—Ç–µ —Å–∞ –≥–æ—Ç–æ–≤–∏ –∑–∞ production."
            )
        
        summary = analysis.get('metrics_summary', {})
        avg_accuracy = summary.get('avg_accuracy', 0)
        
        if avg_accuracy < 0.6:
            recommendations.append(
                f"‚ö†Ô∏è –°—Ä–µ–¥–Ω–∏—è—Ç accuracy ({avg_accuracy:.3f}) –µ –ø–æ–¥ 60%. "
                f"–ü—Ä–µ–ø–æ—Ä—ä—á–≤–∞ —Å–µ –ø—Ä–µ—Ä–∞–∑–≥–ª–µ–∂–¥–∞–Ω–µ –Ω–∞ feature selection."
            )
        elif avg_accuracy > 0.8:
            recommendations.append(
                f"‚úÖ –û—Ç–ª–∏—á–µ–Ω —Å—Ä–µ–¥–µ–Ω accuracy ({avg_accuracy:.3f}). "
                f"–ú–æ–¥–µ–ª–∏—Ç–µ —Ä–∞–±–æ—Ç—è—Ç –º–Ω–æ–≥–æ –¥–æ–±—Ä–µ."
            )
        
        return recommendations
    
    def generate_performance_report(self, analysis: Dict, output_file: str = None) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–∞ performance –æ—Ç—á–µ—Ç
        
        Args:
            analysis: –ê–Ω–∞–ª–∏–∑ –Ω–∞ performance
            output_file: –§–∞–π–ª –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ
        
        Returns:
            –¢–µ–∫—Å—Ç–æ–≤ –æ—Ç—á–µ—Ç
        """
        report_lines = [
            "üìä PERFORMANCE –ú–û–ù–ò–¢–û–†–ò–ù–ì: PER-LEAGUE OU2.5 –ú–û–î–ï–õ–ò",
            "=" * 70,
            f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            ""
        ]
        
        # Summary
        summary = analysis.get('metrics_summary', {})
        if summary:
            report_lines.extend([
                "üìà –û–ë–© –ü–†–ï–ì–õ–ï–î:",
                f"   –û–±—â–æ –ª–∏–≥–∏: {summary['total_leagues']}",
                f"   –°—Ä–µ–¥–µ–Ω score: {summary['avg_score']:.1f}/100",
                f"   –ú–µ–¥–∏–∞–Ω score: {summary['median_score']:.1f}/100",
                f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {summary['std_score']:.1f}",
                f"   –°—Ä–µ–¥–µ–Ω accuracy: {summary['avg_accuracy']:.3f}",
                f"   –°—Ä–µ–¥–µ–Ω log loss: {summary['avg_log_loss']:.3f}",
                f"   –°—Ä–µ–¥–µ–Ω Brier score: {summary['avg_brier_score']:.3f}",
                ""
            ])
        
        # Performance tiers
        tiers = analysis.get('performance_tiers', {})
        if any(tiers.values()):
            report_lines.extend([
                "üèÜ PERFORMANCE –ö–ê–¢–ï–ì–û–†–ò–ò:",
                ""
            ])
            
            tier_info = [
                ("üü¢ –û—Ç–ª–∏—á–Ω–∏ (80-100)", tiers['excellent']),
                ("üü° –î–æ–±—Ä–∏ (70-79)", tiers['good']),
                ("üü† –°—Ä–µ–¥–Ω–∏ (60-69)", tiers['average']),
                ("üî¥ –°–ª–∞–±–∏ (<60)", tiers['poor'])
            ]
            
            for tier_name, leagues in tier_info:
                if leagues:
                    league_names = [get_league_display_name(l) for l in leagues]
                    report_lines.extend([
                        f"{tier_name}: {len(leagues)} –ª–∏–≥–∏",
                        f"   {', '.join(league_names)}",
                        ""
                    ])
        
        # League rankings
        rankings = analysis.get('league_rankings', {})
        if rankings:
            report_lines.extend([
                "ü•á –ö–õ–ê–°–ò–†–ê–ù–ï –ü–û PERFORMANCE:",
                ""
            ])
            
            for i, (league_slug, data) in enumerate(rankings.items(), 1):
                medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i:2d}."
                
                report_lines.extend([
                    f"{medal} {data['name']}:",
                    f"    Score: {data['score']:.1f}/100",
                    f"    Accuracy: {data['accuracy']:.3f}",
                    f"    Log Loss: {data['log_loss']:.3f}",
                    f"    Brier Score: {data['brier_score']:.3f}",
                    f"    –ú–∞—á–æ–≤–µ: {data['matches']}",
                    f"    –ö–∞–ª–∏–±—Ä–∏—Ä–∞–Ω: {'‚úÖ' if data['calibrated'] else '‚ùå'}",
                    ""
                ])
        
        # –ü—Ä–µ–ø–æ—Ä—ä–∫–∏
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            report_lines.extend([
                "üí° –ü–†–ï–ü–û–†–™–ö–ò:",
                ""
            ])
            
            for rec in recommendations:
                report_lines.append(f"   {rec}")
                report_lines.append("")
        
        report = "\n".join(report_lines)
        
        if output_file:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            self.logger.info(f"üìÑ Performance –æ—Ç—á–µ—Ç –∑–∞–ø–∞–∑–µ–Ω –≤ {output_file}")
        
        return report
    
    def create_performance_dashboard(self, analysis: Dict, output_dir: str = "reports/performance"):
        """
        –°—ä–∑–¥–∞–≤–∞ performance dashboard —Å –≥—Ä–∞—Ñ–∏–∫–∏
        
        Args:
            analysis: –ê–Ω–∞–ª–∏–∑ –Ω–∞ performance
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ
        """
        self.logger.info("üìä –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ performance dashboard...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        rankings = analysis.get('league_rankings', {})
        if not rankings:
            self.logger.warning("–ù—è–º–∞ –¥–∞–Ω–Ω–∏ –∑–∞ dashboard")
            return
        
        # –ü–æ–¥–≥–æ—Ç–≤—è –¥–∞–Ω–Ω–∏
        leagues = []
        scores = []
        accuracies = []
        log_losses = []
        brier_scores = []
        matches = []
        
        for league_slug, data in rankings.items():
            leagues.append(data['name'])
            scores.append(data['score'])
            accuracies.append(data['accuracy'])
            log_losses.append(data['log_loss'])
            brier_scores.append(data['brier_score'])
            matches.append(data['matches'])
        
        # –°—Ç–∏–ª
        plt.style.use('default')
        sns.set_palette("husl")
        
        # 1. Overall Performance Score
        fig, ax = plt.subplots(figsize=(12, 8))
        bars = ax.barh(leagues, scores, color=sns.color_palette("RdYlGn", len(leagues)))
        ax.set_xlabel('Performance Score')
        ax.set_title('Per-League OU2.5 Models Performance Ranking', fontsize=16, fontweight='bold')
        ax.set_xlim(0, 100)
        
        # –î–æ–±–∞–≤—è —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ –Ω–∞ –±–∞—Ä–æ–≤–µ
        for i, (bar, score) in enumerate(zip(bars, scores)):
            ax.text(score + 1, bar.get_y() + bar.get_height()/2, 
                   f'{score:.1f}', va='center', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/performance_ranking.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Metrics Comparison
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # Accuracy
        ax1.bar(leagues, accuracies, color='skyblue', alpha=0.7)
        ax1.set_title('Validation Accuracy by League', fontweight='bold')
        ax1.set_ylabel('Accuracy')
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(axis='y', alpha=0.3)
        
        # Log Loss
        ax2.bar(leagues, log_losses, color='lightcoral', alpha=0.7)
        ax2.set_title('Validation Log Loss by League', fontweight='bold')
        ax2.set_ylabel('Log Loss')
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(axis='y', alpha=0.3)
        
        # Brier Score
        ax3.bar(leagues, brier_scores, color='lightgreen', alpha=0.7)
        ax3.set_title('Validation Brier Score by League', fontweight='bold')
        ax3.set_ylabel('Brier Score')
        ax3.tick_params(axis='x', rotation=45)
        ax3.grid(axis='y', alpha=0.3)
        
        # Training Data Size
        ax4.bar(leagues, matches, color='gold', alpha=0.7)
        ax4.set_title('Training Data Size by League', fontweight='bold')
        ax4.set_ylabel('Number of Matches')
        ax4.tick_params(axis='x', rotation=45)
        ax4.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/metrics_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Performance Tiers Pie Chart
        tiers = analysis.get('performance_tiers', {})
        tier_counts = {k: len(v) for k, v in tiers.items() if v}
        
        if tier_counts:
            fig, ax = plt.subplots(figsize=(10, 8))
            
            labels = []
            sizes = []
            colors = []
            
            tier_info = [
                ('excellent', '–û—Ç–ª–∏—á–Ω–∏ (80-100)', '#2ecc71'),
                ('good', '–î–æ–±—Ä–∏ (70-79)', '#f1c40f'),
                ('average', '–°—Ä–µ–¥–Ω–∏ (60-69)', '#e67e22'),
                ('poor', '–°–ª–∞–±–∏ (<60)', '#e74c3c')
            ]
            
            for tier_key, tier_label, color in tier_info:
                if tier_key in tier_counts:
                    labels.append(f"{tier_label}\n({tier_counts[tier_key]} –ª–∏–≥–∏)")
                    sizes.append(tier_counts[tier_key])
                    colors.append(color)
            
            wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, 
                                            autopct='%1.1f%%', startangle=90)
            ax.set_title('Performance Distribution', fontsize=16, fontweight='bold')
            
            plt.savefig(f"{output_dir}/performance_distribution.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        self.logger.info(f"üìä Dashboard –∑–∞–ø–∞–∑–µ–Ω –≤ {output_dir}/")


def main():
    """–ì–ª–∞–≤–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞ performance –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"""
    logger = setup_logging()
    
    logger.info("üìä –°–¢–ê–†–¢–ò–†–ê–ù–ï –ù–ê PERFORMANCE –ú–û–ù–ò–¢–û–†–ò–ù–ì")
    logger.info("=" * 60)
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞ monitor
        monitor = PerLeaguePerformanceMonitor()
        
        # –ó–∞—Ä–µ–∂–¥–∞ training –º–µ—Ç—Ä–∏–∫–∏
        training_metrics = monitor.load_training_metrics()
        
        if not training_metrics:
            logger.error("‚ùå –ù—è–º–∞ training –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ –∞–Ω–∞–ª–∏–∑")
            return
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä–∞ performance
        analysis = monitor.analyze_model_performance(training_metrics)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä–∞ –æ—Ç—á–µ—Ç
        report = monitor.generate_performance_report(
            analysis, 
            output_file="reports/performance_monitoring_report.txt"
        )
        
        print(report)
        
        # –°—ä–∑–¥–∞–≤–∞ dashboard
        monitor.create_performance_dashboard(analysis)
        
        # –ó–∞–ø–∞–∑–≤–∞ –∞–Ω–∞–ª–∏–∑–∞
        analysis_file = "reports/performance_analysis.json"
        os.makedirs(os.path.dirname(analysis_file), exist_ok=True)
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'analysis': analysis,
                'training_metrics': training_metrics
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ –ê–Ω–∞–ª–∏–∑ –∑–∞–ø–∞–∑–µ–Ω –≤ {analysis_file}")
        
        # –°—Ç–∞—Ä—Ç–∏—Ä–∞ adaptive learning cycle
        try:
            logger.info("ü§ñ –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ adaptive learning cycle...")
            from pipelines.adaptive_trainer import AdaptiveTrainer
            
            adaptive_trainer = AdaptiveTrainer()
            adaptive_results = adaptive_trainer.adaptive_learning_cycle()
            
            if adaptive_results.get('enabled', False):
                summary = adaptive_results.get('summary', {})
                logger.info(
                    f"ü§ñ Adaptive learning –∑–∞–≤—ä—Ä—à–µ–Ω: "
                    f"{summary.get('total_retrained', 0)} retrained –ª–∏–≥–∏, "
                    f"success rate: {summary.get('success_rate', 0):.1%}"
                )
                
                # –î–æ–±–∞–≤—è adaptive —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ –∫—ä–º –∞–Ω–∞–ª–∏–∑–∞
                analysis['adaptive_learning'] = adaptive_results
                
                # –û–±–Ω–æ–≤—è–≤–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'timestamp': datetime.now().isoformat(),
                        'analysis': analysis,
                        'training_metrics': training_metrics,
                        'adaptive_results': adaptive_results
                    }, f, indent=2, ensure_ascii=False)
            else:
                logger.info("üîí Adaptive learning –µ –∏–∑–∫–ª—é—á–µ–Ω")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ì—Ä–µ—à–∫–∞ –≤ adaptive learning: {e}")
        
        # –°—Ç–∞—Ä—Ç–∏—Ä–∞ ensemble weights optimization
        try:
            logger.info("üéØ –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ ensemble weights optimization...")
            from pipelines.ensemble_optimizer import optimize_ensemble_weights
            
            ensemble_results = optimize_ensemble_weights()
            
            if ensemble_results.get('enabled', False):
                if ensemble_results.get('success', False):
                    if ensemble_results.get('weights_updated', False):
                        metrics = ensemble_results.get('metrics', {})
                        improvement = metrics.get('improvement', 0)
                        logger.info(
                            f"üéØ Ensemble optimization —É—Å–ø–µ—à–µ–Ω: "
                            f"{improvement:.1%} –ø–æ–¥–æ–±—Ä–µ–Ω–∏–µ –≤ log_loss"
                        )
                    else:
                        logger.info("üìä Ensemble —Ç–µ–≥–ª–∞ –Ω–µ —Å–∞ –ø—Ä–æ–º–µ–Ω–µ–Ω–∏ (–Ω–µ–¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ –ø–æ–¥–æ–±—Ä–µ–Ω–∏–µ)")
                else:
                    error = ensemble_results.get('error', 'Unknown error')
                    logger.warning(f"‚ö†Ô∏è Ensemble optimization –Ω–µ—É—Å–ø–µ—à–µ–Ω: {error}")
                
                # –î–æ–±–∞–≤—è ensemble —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ –∫—ä–º –∞–Ω–∞–ª–∏–∑–∞
                analysis['ensemble_optimization'] = ensemble_results
                
                # –û–±–Ω–æ–≤—è–≤–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞
                def convert_for_json(obj):
                    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–∞ numpy —Ç–∏–ø–æ–≤–µ –∑–∞ JSON serialization"""
                    import numpy as np
                    if isinstance(obj, np.bool_):
                        return bool(obj)
                    elif isinstance(obj, np.integer):
                        return int(obj)
                    elif isinstance(obj, np.floating):
                        return float(obj)
                    elif isinstance(obj, dict):
                        return {k: convert_for_json(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [convert_for_json(item) for item in obj]
                    return obj
                
                analysis_data = {
                    'timestamp': datetime.now().isoformat(),
                    'analysis': analysis,
                    'training_metrics': training_metrics,
                    'adaptive_results': adaptive_results if 'adaptive_results' in locals() else None,
                    'ensemble_results': ensemble_results
                }
                
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(convert_for_json(analysis_data), f, indent=2, ensure_ascii=False)
            else:
                logger.info("üîí Ensemble optimization –µ –∏–∑–∫–ª—é—á–µ–Ω–∞")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ì—Ä–µ—à–∫–∞ –≤ ensemble optimization: {e}")
        
        # –°—Ç–∞—Ä—Ç–∏—Ä–∞ advanced drift analysis
        try:
            logger.info("üîç –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ advanced drift analysis...")
            from pipelines.drift_analyzer import run_drift_analysis
            
            drift_results = run_drift_analysis()
            
            if drift_results.get('enabled', False):
                if drift_results.get('success', False):
                    drift_report = drift_results.get('drift_report', {})
                    overall_drift = drift_report.get('overall_drift', {})
                    
                    severity = overall_drift.get('severity', 'none')
                    score = overall_drift.get('score', 0)
                    detected = overall_drift.get('detected', False)
                    
                    if detected:
                        if severity == 'critical':
                            logger.critical(f"üö® –ö–†–ò–¢–ò–ß–ï–ù DRIFT –æ—Ç–∫—Ä–∏—Ç: score={score:.3f}")
                        elif severity == 'high':
                            logger.warning(f"‚ö†Ô∏è –í–ò–°–û–ö DRIFT –æ—Ç–∫—Ä–∏—Ç: score={score:.3f}")
                        elif severity == 'medium':
                            logger.info(f"üìà –°–†–ï–î–ï–ù DRIFT –æ—Ç–∫—Ä–∏—Ç: score={score:.3f}")
                        else:
                            logger.info(f"üìä –ù–ò–°–™–ö DRIFT –æ—Ç–∫—Ä–∏—Ç: score={score:.3f}")
                        
                        # –ú–∞—Ä–∫–∏—Ä–∞ high-risk –∑–æ–Ω–∏
                        drift_types = drift_report.get('drift_types', {})
                        league_drift = drift_types.get('league_drift', {})
                        
                        if league_drift.get('detected', False):
                            league_details = league_drift.get('details', {}).get('leagues', {})
                            high_risk_leagues = [
                                league for league, info in league_details.items()
                                if info.get('drift_detected', False)
                            ]
                            
                            if high_risk_leagues:
                                logger.warning(f"üéØ High-risk –ª–∏–≥–∏: {', '.join(high_risk_leagues)}")
                        
                        # Trigger adaptive learning –ø—Ä–∏ high drift
                        integration_config = drift_results.get('drift_report', {}).get('integration', {})
                        if (severity in ['high', 'critical'] and 
                            integration_config.get('trigger_adaptive_learning', True)):
                            logger.info("üîÑ Triggering adaptive learning –∑–∞—Ä–∞–¥–∏ drift...")
                            
                            # –ó–∞–ø–∏—Å–≤–∞ drift –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ adaptive learning history
                            try:
                                history_file = "logs/adaptive_learning_history.json"
                                
                                # –ó–∞—Ä–µ–∂–¥–∞ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è
                                if os.path.exists(history_file):
                                    with open(history_file, 'r', encoding='utf-8') as f:
                                        history_data = json.load(f)
                                        # –ê–∫–æ –µ dict, –∏–∑–≤–ª–∏—á–∞ –∏—Å—Ç–æ—Ä–∏—è—Ç–∞
                                        if isinstance(history_data, dict):
                                            history = history_data.get('history', [])
                                        else:
                                            history = history_data if isinstance(history_data, list) else []
                                else:
                                    history = []
                                
                                # –î–æ–±–∞–≤—è drift –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                                drift_entry = {
                                    'timestamp': datetime.now().isoformat(),
                                    'type': 'drift_detection',
                                    'severity': severity,
                                    'drift_score': score,
                                    'high_risk_leagues': high_risk_leagues if 'high_risk_leagues' in locals() else [],
                                    'recommendations': drift_report.get('recommendations', [])
                                }
                                
                                history.append(drift_entry)
                                
                                # –ó–∞–ø–∞–∑–≤–∞ –æ–±–Ω–æ–≤–µ–Ω–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è
                                with open(history_file, 'w', encoding='utf-8') as f:
                                    json.dump(history, f, indent=2, ensure_ascii=False)
                                
                                logger.info(f"üíæ Drift –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞–ø–∏—Å–∞–Ω–∞ –≤ {history_file}")
                                
                            except Exception as hist_e:
                                logger.error(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å –Ω–∞ drift –∏—Å—Ç–æ—Ä–∏—è: {hist_e}")
                    else:
                        logger.info(f"‚úÖ –ù—è–º–∞ –∑–Ω–∞—á–∏—Ç–µ–ª–µ–Ω drift: score={score:.3f}")
                else:
                    error = drift_results.get('error', 'Unknown error')
                    logger.warning(f"‚ö†Ô∏è Drift analysis –Ω–µ—É—Å–ø–µ—à–µ–Ω: {error}")
                
                # –î–æ–±–∞–≤—è drift —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ –∫—ä–º –∞–Ω–∞–ª–∏–∑–∞
                analysis['drift_analysis'] = drift_results
                
                # –û–±–Ω–æ–≤—è–≤–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞
                analysis_data = {
                    'timestamp': datetime.now().isoformat(),
                    'analysis': analysis,
                    'training_metrics': training_metrics,
                    'adaptive_results': adaptive_results if 'adaptive_results' in locals() else None,
                    'ensemble_results': ensemble_results if 'ensemble_results' in locals() else None,
                    'drift_results': drift_results
                }
                
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(convert_for_json(analysis_data), f, indent=2, ensure_ascii=False)
            else:
                logger.info("üîí Drift analysis –µ –∏–∑–∫–ª—é—á–µ–Ω")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ì—Ä–µ—à–∫–∞ –≤ drift analysis: {e}")
        
        # –°—Ç–∞—Ä—Ç–∏—Ä–∞ league ensemble optimization
        try:
            logger.info("üéØ –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ league ensemble optimization...")
            from pipelines.league_ensemble_optimizer import run_league_ensemble_optimization
            
            league_ensemble_results = run_league_ensemble_optimization()
            
            if league_ensemble_results.get('enabled', False):
                if league_ensemble_results.get('success', False):
                    leagues_optimized = league_ensemble_results.get('leagues_optimized', 0)
                    leagues_updated = league_ensemble_results.get('leagues_updated', 0)
                    
                    if leagues_updated > 0:
                        success_rate = leagues_updated / leagues_optimized * 100 if leagues_optimized > 0 else 0
                        logger.info(
                            f"üéØ League ensemble optimization —É—Å–ø–µ—à–µ–Ω: "
                            f"{leagues_updated}/{leagues_optimized} –ª–∏–≥–∏ –æ–±–Ω–æ–≤–µ–Ω–∏ ({success_rate:.1f}%)"
                        )
                        
                        # –ü–æ–∫–∞–∑–≤–∞ —Ç–æ–ø –ø–æ–¥–æ–±—Ä–µ–Ω–∏—è
                        league_results = league_ensemble_results.get('league_results', {})
                        if league_results:
                            top_improvements = sorted(
                                [(league, data['metrics']['improvement']) 
                                 for league, data in league_results.items()],
                                key=lambda x: x[1], reverse=True
                            )[:3]  # –¢–æ–ø 3
                            
                            logger.info("üèÜ –¢–æ–ø –ø–æ–¥–æ–±—Ä–µ–Ω–∏—è:")
                            for league, improvement in top_improvements:
                                logger.info(f"   {league}: {improvement:.1%}")
                    else:
                        logger.info("üìä –ù—è–º–∞ –ª–∏–≥–∏ —Å –¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ –ø–æ–¥–æ–±—Ä–µ–Ω–∏–µ –∑–∞ –æ–±–Ω–æ–≤—è–≤–∞–Ω–µ")
                else:
                    error = league_ensemble_results.get('error', 'Unknown error')
                    logger.warning(f"‚ö†Ô∏è League ensemble optimization –Ω–µ—É—Å–ø–µ—à–µ–Ω: {error}")
                
                # –î–æ–±–∞–≤—è league ensemble —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ –∫—ä–º –∞–Ω–∞–ª–∏–∑–∞
                analysis['league_ensemble_optimization'] = league_ensemble_results
                
                # –û–±–Ω–æ–≤—è–≤–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞
                analysis_data = {
                    'timestamp': datetime.now().isoformat(),
                    'analysis': analysis,
                    'training_metrics': training_metrics,
                    'adaptive_results': adaptive_results if 'adaptive_results' in locals() else None,
                    'ensemble_results': ensemble_results if 'ensemble_results' in locals() else None,
                    'drift_results': drift_results if 'drift_results' in locals() else None,
                    'league_ensemble_results': league_ensemble_results
                }
                
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(convert_for_json(analysis_data), f, indent=2, ensure_ascii=False)
            else:
                logger.info("üîí League ensemble optimization –µ –∏–∑–∫–ª—é—á–µ–Ω–∞")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ì—Ä–µ—à–∫–∞ –≤ league ensemble optimization: {e}")
        
        logger.info("‚úÖ Performance –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≤—ä—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
    except Exception as e:
        logger.error(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ performance –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: {e}")
        raise


if __name__ == "__main__":
    main()
