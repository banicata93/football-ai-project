"""
üéØ –ê–ù–ê–õ–ò–ó –ù–ê –í–ï–†–û–Ø–¢–ù–û–°–¢–ù–ê–¢–ê –õ–û–ì–ò–ö–ê - Football AI Project
–í—Å–∏—á–∫–∏ –∫–ª—é—á–æ–≤–∏ —á–∞—Å—Ç–∏ –æ—Ç –∫–æ–¥–∞ –∑–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –∫–∞–ª–∏–±—Ä–∞—Ü–∏—è –∏ ensemble
"""

# =============================================================================
# 1Ô∏è‚É£ POISSON MODEL - –û—Å–Ω–æ–≤–Ω–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∏ –∏–∑—á–∏—Å–ª–µ–Ω–∏—è
# –§–∞–π–ª: core/poisson_utils.py
# =============================================================================

def calculate_lambda(
    self,
    home_team_id: int,
    away_team_id: int,
    league_id: Optional[int] = None
) -> Tuple[float, float]:
    """
    üîç –ö–õ–Æ–ß–û–í–ê –§–£–ù–ö–¶–ò–Ø: –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ Œª (–æ—á–∞–∫–≤–∞–Ω–∏ –≥–æ–ª–æ–≤–µ)
    
    –¢–ï–ö–£–©–ê –§–û–†–ú–£–õ–ê:
    Œª_home = league_avg_home * home_attack * away_defense * home_advantage
    Œª_away = league_avg_away * away_attack * home_defense
    
    –ü–†–û–ë–õ–ï–ú–ò:
    - home_advantage = 1.15 (—Ñ–∏–∫—Å–∏—Ä–∞–Ω) - –º–æ–∂–µ –¥–∞ –µ —Ç–≤—ä—Ä–¥–µ –≤–∏—Å–æ–∫
    - –ù—è–º–∞ league-specific home advantage
    - –ù—è–º–∞ momentum/form adjustment
    """
    # League average
    if league_id and league_id in self.league_avg_goals_home:
        avg_home = self.league_avg_goals_home[league_id]
        avg_away = self.league_avg_goals_away[league_id]
    else:
        avg_home = self.league_avg_goals_home.get(0, 1.5)  # üö® Default 1.5
        avg_away = self.league_avg_goals_away.get(0, 1.2)  # üö® Default 1.2
    
    # Team strengths
    home_attack = self.attack_strength.get(home_team_id, 1.0)   # üö® Default 1.0
    home_defense = self.defense_strength.get(home_team_id, 1.0)
    away_attack = self.attack_strength.get(away_team_id, 1.0)
    away_defense = self.defense_strength.get(away_team_id, 1.0)
    
    # üéØ –ö–†–ò–¢–ò–ß–ù–ò –§–û–†–ú–£–õ–ò:
    lambda_home = avg_home * home_attack * away_defense * self.home_advantage  # 1.15
    lambda_away = avg_away * away_attack * home_defense
    
    return lambda_home, lambda_away


def predict_match_probabilities(
    self,
    home_team_id: int,
    away_team_id: int,
    league_id: Optional[int] = None,
    max_goals: int = 10
) -> Dict[str, float]:
    """
    üîç –ö–õ–Æ–ß–û–í–ê –§–£–ù–ö–¶–ò–Ø: Poisson –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∑–∞ –≤—Å–∏—á–∫–∏ –ø–∞–∑–∞—Ä–∏
    
    –ü–†–û–ë–õ–ï–ú–ò:
    - max_goals=10 –º–æ–∂–µ –¥–∞ –µ –Ω–µ–¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ –∑–∞ –≤–∏—Å–æ–∫–∏ Œª
    - –ù—è–º–∞ –∫–æ—Ä–µ–∫—Ü–∏—è –∑–∞ low-scoring leagues
    - –ù—è–º–∞ weather/venue adjustments
    """
    # –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ lambda
    lambda_home, lambda_away = self.calculate_lambda(
        home_team_id, away_team_id, league_id
    )
    
    # üéØ –ú–ê–¢–†–ò–¶–ê –° –í–ï–†–û–Ø–¢–ù–û–°–¢–ò:
    prob_matrix = np.zeros((max_goals + 1, max_goals + 1))
    
    for i in range(max_goals + 1):
        for j in range(max_goals + 1):
            prob_matrix[i, j] = poisson.pmf(i, lambda_home) * poisson.pmf(j, lambda_away)
    
    # üéØ 1X2 –í–ï–†–û–Ø–¢–ù–û–°–¢–ò:
    prob_home_win = np.sum(np.tril(prob_matrix, -1))  # –ü–æ–¥ –¥–∏–∞–≥–æ–Ω–∞–ª–∞
    prob_draw = np.sum(np.diag(prob_matrix))          # –î–∏–∞–≥–æ–Ω–∞–ª
    prob_away_win = np.sum(np.triu(prob_matrix, 1))   # –ù–∞–¥ –¥–∏–∞–≥–æ–Ω–∞–ª–∞
    
    # üéØ OVER/UNDER 2.5:
    prob_over_25 = 0
    prob_under_25 = 0
    
    for i in range(max_goals + 1):
        for j in range(max_goals + 1):
            if i + j > 2.5:  # üö® –¢–≤—ä—Ä–¥–∞ –≥—Ä–∞–Ω–∏—Ü–∞ 2.5
                prob_over_25 += prob_matrix[i, j]
            else:
                prob_under_25 += prob_matrix[i, j]
    
    # üéØ BTTS (Both Teams To Score):
    prob_btts_yes = 0
    prob_btts_no = 0
    
    for i in range(max_goals + 1):
        for j in range(max_goals + 1):
            if i > 0 and j > 0:  # üö® –ò –¥–≤–∞—Ç–∞ > 0
                prob_btts_yes += prob_matrix[i, j]
            else:
                prob_btts_no += prob_matrix[i, j]
    
    return {
        'lambda_home': lambda_home,
        'lambda_away': lambda_away,
        'prob_home_win': prob_home_win,
        'prob_draw': prob_draw,
        'prob_away_win': prob_away_win,
        'prob_over_25': prob_over_25,
        'prob_under_25': prob_under_25,
        'prob_btts_yes': prob_btts_yes,
        'prob_btts_no': prob_btts_no,
        'expected_home_goals': lambda_home,
        'expected_away_goals': lambda_away,
        'expected_total_goals': lambda_home + lambda_away
    }


# =============================================================================
# 2Ô∏è‚É£ ML MODELS - predict_proba() –∏–∑–≤–∏–∫–≤–∞–Ω–∏—è –∏ –∫–∞–ª–∏–±—Ä–∞—Ü–∏—è
# –§–∞–π–ª: api/prediction_service.py
# =============================================================================

def ml_predictions_and_calibration():
    """
    üîç ML –ú–û–î–ï–õ–ò: –ö–∞–∫ —Å–µ –ø—Ä–∞–≤—è—Ç predictions –∏ calibration
    
    –ü–†–û–ë–õ–ï–ú–ò:
    - –°–∞–º–æ BTTS –∏–º–∞ calibration
    - 1X2 –∏ OU2.5 –∏–∑–ø–æ–ª–∑–≤–∞—Ç —Å—É—Ä–æ–≤–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    - –ù—è–º–∞ confidence adjustment
    """
    
    # üéØ ML PREDICTIONS (—Ä–µ–¥–æ–≤–µ 312-314):
    ml_1x2 = self.models['1x2'].predict_proba(X_1x2)[0]        # [prob_1, prob_X, prob_2]
    ml_ou25 = self.models['ou25'].predict_proba(X_ou25)[0, 1]  # prob_over (—Å–∞–º–æ –∫–ª–∞—Å–∞ 1)
    ml_btts_raw = self.models['btts'].predict_proba(X_btts)[0, 1]  # prob_yes (—Å–∞–º–æ –∫–ª–∞—Å–∞ 1)
    
    # üéØ BTTS CALIBRATION (—Ä–µ–¥–æ–≤–µ 316-318):
    # üö® –ó–ê–©–û –°–ê–ú–û BTTS –ò–ú–ê CALIBRATION?
    ml_btts_calibrated = 0.5 + (ml_btts_raw - 0.5) * 0.85  # –ù–∞–º–∞–ª—è–≤–∞ overconfidence —Å 15%
    ml_btts_calibrated = np.clip(ml_btts_calibrated, 0.05, 0.95)  # Clipping
    
    # üéØ BLENDING –° POISSON (—Ä–µ–¥ 321):
    ml_btts = 0.8 * ml_btts_calibrated + 0.2 * poisson_pred['prob_btts']
    # üö® –ó–ê–©–û 80%-20% SPLIT? –û–ü–¢–ò–ú–ò–ó–ò–†–ê–ù–û –õ–ò –ï?
    
    return ml_1x2, ml_ou25, ml_btts


# =============================================================================
# 3Ô∏è‚É£ ENSEMBLE MODEL - –ö–æ–º–±–∏–Ω–∏—Ä–∞–Ω–µ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
# –§–∞–π–ª: core/ensemble.py
# =============================================================================

class EnsembleModel:
    """
    üîç ENSEMBLE: –ö–æ–º–±–∏–Ω–∏—Ä–∞–Ω–µ –Ω–∞ Poisson, ML –∏ Elo predictions
    
    –ü–†–û–ë–õ–ï–ú–ò:
    - –§–∏–∫—Å–∏—Ä–∞–Ω–∏ weights –±–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    - –ù—è–º–∞ dynamic weighting —Å–ø–æ—Ä–µ–¥ confidence
    - –ù—è–º–∞ league-specific weights
    """
    
    def __init__(self, initial_weights: Optional[Dict[str, float]] = None):
        # üéØ DEFAULT WEIGHTS (—Ä–µ–¥–æ–≤–µ 38-42):
        self.weights = initial_weights or {
            'poisson': 0.3,  # üö® 30% - –ó–∞—â–æ —Ç–æ–ª–∫–æ–≤–∞ –Ω–∏—Å–∫–æ?
            'ml': 0.5,       # üö® 50% - –ù–∞–π-–≤–∏—Å–æ–∫ weight
            'elo': 0.2       # üö® 20% - –ù–∞–π-–Ω–∏—Å—ä–∫ weight
        }
        # üö® –¢–ï–ó–ò WEIGHTS –°–ê –§–ò–ö–°–ò–†–ê–ù–ò! –ù–Ø–ú–ê –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –í PRODUCTION!
    
    def optimize_weights(self, predictions: Dict[str, np.ndarray], y_true: np.ndarray):
        """
        üîç WEIGHT OPTIMIZATION: –ú–∏–Ω–∏–º–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ log loss
        
        –ü–†–û–ë–õ–ï–ú–ò:
        - –ò–∑–ø–æ–ª–∑–≤–∞ —Å–µ —Å–∞–º–æ –≤ training, –Ω–µ –≤ production
        - –ù—è–º–∞ regularization
        - –ù—è–º–∞ cross-validation
        """
        # Objective function (—Ä–µ–¥–æ–≤–µ 75-88):
        def objective(weights):
            combined = self._combine_predictions(predictions, weights)
            combined = np.clip(combined, 1e-15, 1 - 1e-15)  # üö® Hard clipping
            
            # Normalization (—Ä–µ–¥–æ–≤–µ 82-83):
            if combined.ndim == 2:
                combined = combined / combined.sum(axis=1, keepdims=True)
            
            return log_loss(y_true, combined)
        
        # üö® CONSTRAINTS: weights sum to 1, bounds [0,1]
        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
        bounds = [(0, 1) for _ in range(len(predictions))]
    
    def _combine_predictions(self, predictions: Dict[str, np.ndarray], weights: Optional[np.ndarray] = None):
        """
        üîç PREDICTION COMBINING: Weighted average
        
        –ü–†–û–ë–õ–ï–ú–ò:
        - –°–∞–º–æ linear combination
        - –ù—è–º–∞ non-linear blending
        - –ù—è–º–∞ confidence weighting
        """
        if weights is None:
            weights = np.array([self.weights.get(k, 0.33) for k in predictions.keys()])
        
        # üéØ WEIGHTED AVERAGE (—Ä–µ–¥–æ–≤–µ 134-135):
        stacked = np.stack(list(predictions.values()), axis=-1)
        combined = np.average(stacked, axis=-1, weights=weights)
        # üö® –°–ê–ú–û LINEAR COMBINATION!
        
        return combined


# =============================================================================
# 4Ô∏è‚É£ ENSEMBLE INFERENCE - –§–∏–Ω–∞–ª–Ω–∏ predictions –≤ API
# –§–∞–π–ª: api/prediction_service.py
# =============================================================================

def ensemble_inference():
    """
    üîç ENSEMBLE INFERENCE: –ö–∞–∫ —Å–µ –ø—Ä–∞–≤—è—Ç —Ñ–∏–Ω–∞–ª–Ω–∏—Ç–µ predictions
    
    –ü–†–û–ë–õ–ï–ú–ò:
    - –†–∞–∑–ª–∏—á–Ω–∏ ensemble –º–µ—Ç–æ–¥–∏ –∑–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ –ø–∞–∑–∞—Ä–∏
    - –ù—è–º–∞ consistency –º–µ–∂–¥—É –ø–∞–∑–∞—Ä–∏—Ç–µ
    - Confidence —Å–µ –∏–∑—á–∏—Å–ª—è–≤–∞ –∫–∞—Ç–æ max probability
    """
    
    # üéØ 1X2 ENSEMBLE (—Ä–µ–¥–æ–≤–µ 324-327):
    ensemble_1x2 = self.models['ensemble'].predict(
        poisson_pred['probs_1x2'].reshape(1, -1),  # [prob_1, prob_X, prob_2]
        ml_1x2.reshape(1, -1)                      # [prob_1, prob_X, prob_2]
    )[0]
    
    # üéØ OU2.5 ENSEMBLE (—Ä–µ–¥–æ–≤–µ 329-332):
    ensemble_ou25 = self.models['ensemble'].predict(
        np.array([[poisson_pred['prob_over25']]]),  # –°–∞–º–æ prob_over
        np.array([[ml_ou25]])                       # –°–∞–º–æ prob_over
    )[0, 0]
    
    # üéØ BTTS ENSEMBLE (—Ä–µ–¥–æ–≤–µ 334-337):
    ensemble_btts = self.models['ensemble'].predict(
        np.array([[poisson_pred['prob_btts']]]),    # –°–∞–º–æ prob_yes
        np.array([[ml_btts]])                       # –°–∞–º–æ prob_yes (–≤–µ—á–µ calibrated)
    )[0, 0]
    
    # üéØ –§–ò–ù–ê–õ–ù–ò –†–ï–ó–£–õ–¢–ê–¢–ò (—Ä–µ–¥–æ–≤–µ 356-376):
    result = {
        'prediction_1x2': {
            'prob_home_win': float(ensemble_1x2[0]),
            'prob_draw': float(ensemble_1x2[1]),
            'prob_away_win': float(ensemble_1x2[2]),
            'predicted_outcome': ['1', 'X', '2'][np.argmax(ensemble_1x2)],
            'confidence': float(np.max(ensemble_1x2))  # üö® MAX PROBABILITY = CONFIDENCE?
        },
        'prediction_ou25': {
            'prob_over': float(ensemble_ou25),
            'prob_under': float(1 - ensemble_ou25),
            'predicted_outcome': 'Over' if ensemble_ou25 > 0.5 else 'Under',  # üö® Hard 0.5 threshold
            'confidence': float(max(ensemble_ou25, 1 - ensemble_ou25))
        },
        'prediction_btts': {
            'prob_yes': float(ensemble_btts),
            'prob_no': float(1 - ensemble_btts),
            'predicted_outcome': self._get_btts_outcome(ensemble_btts, elo_diff),  # üö® Custom logic
            'confidence': float(max(ensemble_btts, 1 - ensemble_btts))
        }
    }


# =============================================================================
# 5Ô∏è‚É£ CALIBRATION & NORMALIZATION - –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∏ –∫–æ—Ä–µ–∫—Ü–∏–∏
# –§–∞–π–ª–æ–≤–µ: pipelines/train_ml_models.py, core/poisson_utils.py
# =============================================================================

def calibration_logic():
    """
    üîç CALIBRATION: –ö–∞–∫ —Å–µ –∫–∞–ª–∏–±—Ä–∏—Ä–∞—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏—Ç–µ
    
    –ü–†–û–ë–õ–ï–ú–ò:
    - –°–∞–º–æ binary models –∏–º–∞—Ç Isotonic Regression
    - 1X2 –Ω—è–º–∞ calibration
    - –†–∞–∑–ª–∏—á–Ω–∏ clipping —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    """
    
    # üéØ ISOTONIC REGRESSION CALIBRATION (train_ml_models.py:334-351):
    from sklearn.calibration import CalibratedClassifierCV
    
    calibrated_model = CalibratedClassifierCV(
        base_model,
        method='isotonic',  # üö® –°–∞–º–æ isotonic, –Ω—è–º–∞ Platt scaling
        cv=3
    )
    calibrated_model.fit(X_val, y_val)
    
    # Calibrated predictions:
    y_train_proba = calibrated_model.predict_proba(X_train)[:, 1]
    y_val_proba = calibrated_model.predict_proba(X_val)[:, 1]
    
    # üéØ POISSON PROBABILITY NORMALIZATION (poisson_utils.py:427-432):
    # 1X2 normalization:
    y_pred_1x2 = np.nan_to_num(y_pred_1x2, nan=0.33)  # üö® Default uniform
    y_pred_1x2 = np.clip(y_pred_1x2, 1e-15, 1 - 1e-15)  # üö® Hard clipping
    
    # Ensure sum to 1:
    row_sums = y_pred_1x2.sum(axis=1, keepdims=True)
    y_pred_1x2 = y_pred_1x2 / row_sums
    
    # Binary clipping:
    y_pred_over25 = np.clip(y_pred_over25, 1e-15, 1 - 1e-15)
    y_pred_btts = np.clip(y_pred_btts, 1e-15, 1 - 1e-15)


# =============================================================================
# 6Ô∏è‚É£ CONFIDENCE SCORING - –ö–∞–∫ —Å–µ –∏–∑—á–∏—Å–ª—è–≤–∞ confidence
# =============================================================================

def confidence_calculation():
    """
    üîç CONFIDENCE: –ö–∞–∫ —Å–µ –∏–∑—á–∏—Å–ª—è–≤–∞ —É–≤–µ—Ä–µ–Ω–æ—Å—Ç—Ç–∞ –≤ –ø—Ä–æ–≥–Ω–æ–∑–∞—Ç–∞
    
    –ü–†–û–ë–õ–ï–ú–ò:
    - Confidence = max(probabilities) - —Ç–≤—ä—Ä–¥–µ –æ–ø—Ä–æ—Å—Ç–µ–Ω–æ
    - –ù—è–º–∞ entropy-based confidence
    - –ù—è–º–∞ model agreement scoring
    """
    
    # üéØ –¢–ï–ö–£–© –ú–ï–¢–û–î:
    confidence_1x2 = float(np.max(ensemble_1x2))  # üö® MAX –æ—Ç [prob_1, prob_X, prob_2]
    confidence_ou25 = float(max(ensemble_ou25, 1 - ensemble_ou25))  # üö® MAX –æ—Ç [prob_over, prob_under]
    confidence_btts = float(max(ensemble_btts, 1 - ensemble_btts))  # üö® MAX –æ—Ç [prob_yes, prob_no]
    
    # üö® –ü–†–û–ë–õ–ï–ú–ò:
    # - High probability != High confidence
    # - –ù–µ –æ—Ç—á–∏—Ç–∞ model disagreement
    # - –ù–µ –æ—Ç—á–∏—Ç–∞ data quality
    # - –ù—è–º–∞ calibration –Ω–∞ confidence scores


# =============================================================================
# 7Ô∏è‚É£ FII (Football Intelligence Index) - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å
# –§–∞–π–ª: core/ensemble.py, api/prediction_service.py
# =============================================================================

def fii_calculation():
    """
    üîç FII: Football Intelligence Index –∑–∞ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞—Ç–∞
    
    –ü–†–û–ë–õ–ï–ú–ò:
    - –û–ø—Ä–æ—Å—Ç–µ–Ω–∞ —Ñ–æ—Ä–º—É–ª–∞
    - –ù—è–º–∞ machine learning –∑–∞ FII
    - –§–∏–∫—Å–∏—Ä–∞–Ω–∏ weights
    """
    
    # üéØ FII COMPONENTS (prediction_service.py:340-346):
    fii_score, fii_conf = self.models['fii'].calculate_fii(
        elo_diff=match_df['elo_diff'].iloc[0],
        form_diff=match_df['home_form_5'].iloc[0] - match_df['away_form_5'].iloc[0],
        xg_efficiency_diff=match_df['home_xg_proxy'].iloc[0] - match_df['away_xg_proxy'].iloc[0],
        finishing_efficiency_diff=match_df['home_shooting_efficiency'].iloc[0] - match_df['away_shooting_efficiency'].iloc[0],
        is_home=1
    )
    
    # üö® FII WEIGHTS (–≤–µ—Ä–æ—è—Ç–Ω–æ —Ñ–∏–∫—Å–∏—Ä–∞–Ω–∏):
    # elo_weight = ?
    # form_weight = ?
    # xg_weight = ?
    # finishing_weight = ?
    # home_weight = ?


# =============================================================================
# 8Ô∏è‚É£ FALLBACK VALUES - Default —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –≥—Ä–µ—à–∫–∏
# =============================================================================

def fallback_values():
    """
    üîç FALLBACK: –ö–∞–∫–≤–∏ default —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ —Å–µ –∏–∑–ø–æ–ª–∑–≤–∞—Ç
    
    –ü–†–û–ë–õ–ï–ú–ò:
    - –¢–≤—ä—Ä–¥–µ –æ–ø—Ä–æ—Å—Ç–µ–Ω–∏ fallbacks
    - –ù—è–º–∞ league-specific defaults
    - –ù—è–º–∞ confidence penalty –∑–∞ fallbacks
    """
    
    # üéØ POISSON FALLBACK (prediction_service.py:284-291):
    poisson_pred = {
        'probs_1x2': np.array([0.33, 0.33, 0.34]),  # üö® Uniform distribution
        'prob_over25': 0.5,                          # üö® 50-50
        'prob_btts': 0.5,                            # üö® 50-50
        'lambda_home': 1.5,                          # üö® League average?
        'lambda_away': 1.2,                          # üö® League average?
        'expected_goals': 2.7                        # üö® 1.5 + 1.2
    }
    
    # üéØ TEAM STRENGTH FALLBACK (poisson_utils.py:134, 174):
    self.attack_strength[team_id] = 1.0   # üö® Average team
    self.defense_strength[team_id] = 1.0  # üö® Average team


# =============================================================================
# üéØ –ö–†–ò–¢–ò–ß–ù–ò –ü–†–û–ë–õ–ï–ú–ò –ó–ê –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø:
# =============================================================================

"""
1Ô∏è‚É£ POISSON BIAS:
   - home_advantage = 1.15 –º–æ–∂–µ –¥–∞ –µ —Ç–≤—ä—Ä–¥–µ –≤–∏—Å–æ–∫
   - –ù—è–º–∞ league-specific adjustments
   - max_goals = 10 –º–æ–∂–µ –¥–∞ –µ –Ω–µ–¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ

2Ô∏è‚É£ ML CALIBRATION:
   - –°–∞–º–æ BTTS –∏–º–∞ calibration (–∑–∞—â–æ?)
   - 1X2 –∏ OU2.5 –∏–∑–ø–æ–ª–∑–≤–∞—Ç —Å—É—Ä–æ–≤–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
   - Calibration –∫–æ–µ—Ñ–∏—Ü–∏–µ–Ω—Ç 0.85 –Ω–µ –µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–∞–Ω

3Ô∏è‚É£ ENSEMBLE WEIGHTS:
   - –§–∏–∫—Å–∏—Ä–∞–Ω–∏ weights: poisson=30%, ml=50%, elo=20%
   - –ù—è–º–∞ dynamic weighting
   - –ù—è–º–∞ league/confidence-based adjustments

4Ô∏è‚É£ CONFIDENCE SCORING:
   - confidence = max(probabilities) –µ —Ç–≤—ä—Ä–¥–µ –æ–ø—Ä–æ—Å—Ç–µ–Ω–æ
   - –ù—è–º–∞ entropy-based confidence
   - –ù—è–º–∞ model agreement scoring

5Ô∏è‚É£ PROBABILITY CLIPPING:
   - Hard clipping 1e-15 –¥–æ 1-1e-15
   - –ú–æ–∂–µ –¥–∞ –≤–ª–æ—à–∞–≤–∞ –∫–∞–ª–∏–±—Ä–∞—Ü–∏—è—Ç–∞
   - –ù—è–º–∞ soft boundaries

6Ô∏è‚É£ NORMALIZATION:
   - –†–∞–∑–ª–∏—á–Ω–∏ –º–µ—Ç–æ–¥–∏ –∑–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ –ø–∞–∑–∞—Ä–∏
   - –ù—è–º–∞ consistency
   - Fallback —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ —Å–∞ —Ç–≤—ä—Ä–¥–µ –æ–ø—Ä–æ—Å—Ç–µ–Ω–∏

7Ô∏è‚É£ FII CALCULATION:
   - –û–ø—Ä–æ—Å—Ç–µ–Ω–∞ —Ñ–æ—Ä–º—É–ª–∞
   - –§–∏–∫—Å–∏—Ä–∞–Ω–∏ weights
   - –ù—è–º–∞ ML-based FII
"""

# =============================================================================
# üöÄ –°–õ–ï–î–í–ê–©–ò –°–¢–™–ü–ö–ò –ó–ê –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø:
# =============================================================================

"""
1. –ö–∞–ª–∏–±—Ä–∞—Ü–∏—è –Ω–∞ Poisson Œª —Ñ–æ—Ä–º—É–ª–∏—Ç–µ
2. ML model calibration –∑–∞ –≤—Å–∏—á–∫–∏ –ø–∞–∑–∞—Ä–∏
3. Dynamic ensemble weighting
4. Entropy-based confidence scoring
5. Soft probability boundaries
6. League-specific adjustments
7. Improved FII calculation
8. Bias reduction (Over 2.5, Home Win)
"""
