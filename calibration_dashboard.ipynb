{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Calibration Dashboard\n",
    "\n",
    "Interactive dashboard Ð·Ð° Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð½Ð° ÐºÐ°Ð»Ð¸Ð±Ñ€Ð°Ñ†Ð¸ÑÑ‚Ð° Ð½Ð° Football AI Ð¼Ð¾Ð´ÐµÐ»Ð°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ð”Ð¾Ð±Ð°Ð²Ñ path ÐºÑŠÐ¼ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°\n",
    "sys.path.append('..')\n",
    "\n",
    "from monitoring.calibration_metrics import CalibrationMonitor, evaluate_calibration\n",
    "from monitoring.adaptive_tuning import AdaptiveTuner\n",
    "\n",
    "# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð·Ð° Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ“Š Calibration Dashboard loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Load Recent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð° Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¸Ñ‚Ðµ\n",
    "monitor = CalibrationMonitor()\n",
    "tuner = AdaptiveTuner()\n",
    "\n",
    "# Ð—Ð°Ñ€ÐµÐ¶Ð´Ð° ÑÐºÐ¾Ñ€Ð¾ÑˆÐ½Ð¸ Ð´Ð°Ð½Ð½Ð¸\n",
    "print(\"ðŸ”„ Loading recent predictions...\")\n",
    "df = monitor.load_recent_predictions(days=30, max_samples=5000)\n",
    "\n",
    "if len(df) == 0:\n",
    "    print(\"âŒ No prediction data found!\")\n",
    "else:\n",
    "    print(f\"âœ… Loaded {len(df)} predictions\")\n",
    "    \n",
    "    # Ð˜Ð·Ñ‡Ð¸ÑÐ»ÑÐ²Ð° Ñ€ÐµÐ°Ð»Ð½Ð¸Ñ‚Ðµ outcomes\n",
    "    df = monitor.calculate_actual_outcomes(df)\n",
    "    \n",
    "    # Ð¤Ð¸Ð»Ñ‚Ñ€Ð¸Ñ€Ð° ÑÐ°Ð¼Ð¾ Ð¼Ð°Ñ‡Ð¾Ð²Ðµ Ñ Ñ€ÐµÐ·ÑƒÐ»Ñ‚Ð°Ñ‚Ð¸\n",
    "    df_with_results = df.dropna(subset=['actual_home_score', 'actual_away_score'])\n",
    "    \n",
    "    print(f\"ðŸ“ˆ {len(df_with_results)} matches with results available\")\n",
    "    \n",
    "    # ÐŸÐ¾ÐºÐ°Ð·Ð²Ð° Ð¾ÑÐ½Ð¾Ð²Ð½Ð° Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ\n",
    "    print(f\"ðŸ“… Date range: {df['prediction_date'].min().date()} to {df['prediction_date'].max().date()}\")\n",
    "    print(f\"ðŸ† Leagues: {df['league'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Current Calibration Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_with_results) > 0:\n",
    "    # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð° ÐºÐ°Ð»Ð¸Ð±Ñ€Ð°Ñ†Ð¸Ð¾Ð½ÐµÐ½ Ð¾Ñ‚Ñ‡ÐµÑ‚\n",
    "    report = monitor.generate_calibration_report(days=30)\n",
    "    \n",
    "    if 'error' not in report:\n",
    "        print(\"ðŸ“Š CURRENT CALIBRATION METRICS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # OU2.5 Metrics\n",
    "        if 'ou25' in report:\n",
    "            ou25 = report['ou25']\n",
    "            print(f\"ðŸŽ¯ OVER/UNDER 2.5:\")\n",
    "            print(f\"   ECE: {ou25['ece']:.4f} ({'âœ… Good' if ou25['ece'] < 0.07 else 'âš ï¸ High'})\")\n",
    "            print(f\"   Brier Score: {ou25['brier_score']:.4f} ({'âœ… Good' if ou25['brier_score'] < 0.20 else 'âš ï¸ High'})\")\n",
    "            print(f\"   Log Loss: {ou25['log_loss']:.4f}\")\n",
    "            print(f\"   Accuracy: {ou25['accuracy']:.3f}\")\n",
    "            print()\n",
    "        \n",
    "        # BTTS Metrics\n",
    "        if 'btts' in report:\n",
    "            btts = report['btts']\n",
    "            print(f\"âš½ BOTH TEAMS TO SCORE:\")\n",
    "            print(f\"   ECE: {btts['ece']:.4f} ({'âœ… Good' if btts['ece'] < 0.07 else 'âš ï¸ High'})\")\n",
    "            print(f\"   Brier Score: {btts['brier_score']:.4f} ({'âœ… Good' if btts['brier_score'] < 0.20 else 'âš ï¸ High'})\")\n",
    "            print(f\"   Log Loss: {btts['log_loss']:.4f}\")\n",
    "            print(f\"   Accuracy: {btts['accuracy']:.3f}\")\n",
    "            print()\n",
    "        \n",
    "        # 1X2 Metrics\n",
    "        if '1x2' in report and 'overall' in report['1x2']:\n",
    "            x12_overall = report['1x2']['overall']\n",
    "            print(f\"ðŸ† 1X2 OVERALL:\")\n",
    "            print(f\"   Brier Score: {x12_overall['brier_score']:.4f} ({'âœ… Good' if x12_overall['brier_score'] < 0.20 else 'âš ï¸ High'})\")\n",
    "            print(f\"   Log Loss: {x12_overall['log_loss']:.4f}\")\n",
    "            print(f\"   Accuracy: {x12_overall['accuracy']:.3f}\")\n",
    "            \n",
    "            # Per-class metrics\n",
    "            for outcome in ['home_win', 'draw', 'away_win']:\n",
    "                if outcome in report['1x2']:\n",
    "                    metrics = report['1x2'][outcome]\n",
    "                    print(f\"   {outcome.title()}: ECE={metrics['ece']:.4f}, Brier={metrics['brier_score']:.4f}\")\n",
    "    else:\n",
    "        print(f\"âŒ Error generating report: {report['error']}\")\n",
    "else:\n",
    "    print(\"âŒ No data with results available for calibration analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Reliability Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_with_results) > 0 and 'error' not in report:\n",
    "    # Ð¡ÑŠÐ·Ð´Ð°Ð²Ð° reliability plots\n",
    "    fig = monitor.plot_reliability_curves(report)\n",
    "    plt.show()\n",
    "    \n",
    "    # Ð—Ð°Ð¿Ð°Ð·Ð²Ð° plot-Ð°\n",
    "    plot_file = f\"reports/calibration/reliability_curves_{datetime.now().strftime('%Y%m%d')}.png\"\n",
    "    os.makedirs(os.path.dirname(plot_file), exist_ok=True)\n",
    "    fig.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ðŸ’¾ Reliability curves saved: {plot_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Interactive Calibration Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_with_results) > 0:\n",
    "    # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð²Ñ Ð´Ð°Ð½Ð½Ð¸ Ð·Ð° trend Ð°Ð½Ð°Ð»Ð¸Ð·\n",
    "    df_with_results['prediction_date'] = pd.to_datetime(df_with_results['prediction_date'])\n",
    "    df_with_results['week'] = df_with_results['prediction_date'].dt.to_period('W')\n",
    "    \n",
    "    # Ð˜Ð·Ñ‡Ð¸ÑÐ»ÑÐ²Ð° ÑÐµÐ´Ð¼Ð¸Ñ‡Ð½Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸\n",
    "    weekly_metrics = []\n",
    "    \n",
    "    for week in df_with_results['week'].unique():\n",
    "        week_data = df_with_results[df_with_results['week'] == week]\n",
    "        \n",
    "        if len(week_data) >= 10:  # ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ 10 Ð¼Ð°Ñ‡Ð° Ð·Ð° ÑÐµÐ´Ð¼Ð¸Ñ†Ð°\n",
    "            week_metrics = {'week': str(week), 'n_matches': len(week_data)}\n",
    "            \n",
    "            # OU2.5 Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸\n",
    "            if 'pred_ou25_prob' in week_data.columns:\n",
    "                ou25_metrics = evaluate_calibration(\n",
    "                    week_data['actual_ou25'].values,\n",
    "                    week_data['pred_ou25_prob'].values\n",
    "                )\n",
    "                week_metrics['ou25_ece'] = ou25_metrics['ece']\n",
    "                week_metrics['ou25_brier'] = ou25_metrics['brier_score']\n",
    "            \n",
    "            # BTTS Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸\n",
    "            if 'pred_btts_prob' in week_data.columns:\n",
    "                btts_metrics = evaluate_calibration(\n",
    "                    week_data['actual_btts'].values,\n",
    "                    week_data['pred_btts_prob'].values\n",
    "                )\n",
    "                week_metrics['btts_ece'] = btts_metrics['ece']\n",
    "                week_metrics['btts_brier'] = btts_metrics['brier_score']\n",
    "            \n",
    "            weekly_metrics.append(week_metrics)\n",
    "    \n",
    "    if weekly_metrics:\n",
    "        weekly_df = pd.DataFrame(weekly_metrics)\n",
    "        \n",
    "        # Ð˜Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²ÐµÐ½ plot Ñ Plotly\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=['OU2.5 ECE Trend', 'OU2.5 Brier Trend', 'BTTS ECE Trend', 'BTTS Brier Trend'],\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # OU2.5 ECE\n",
    "        if 'ou25_ece' in weekly_df.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=weekly_df['week'], y=weekly_df['ou25_ece'], \n",
    "                          mode='lines+markers', name='OU2.5 ECE',\n",
    "                          line=dict(color='blue')),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            fig.add_hline(y=0.07, line_dash=\"dash\", line_color=\"red\", \n",
    "                         annotation_text=\"Threshold\", row=1, col=1)\n",
    "        \n",
    "        # OU2.5 Brier\n",
    "        if 'ou25_brier' in weekly_df.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=weekly_df['week'], y=weekly_df['ou25_brier'], \n",
    "                          mode='lines+markers', name='OU2.5 Brier',\n",
    "                          line=dict(color='green')),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            fig.add_hline(y=0.20, line_dash=\"dash\", line_color=\"red\", \n",
    "                         annotation_text=\"Threshold\", row=1, col=2)\n",
    "        \n",
    "        # BTTS ECE\n",
    "        if 'btts_ece' in weekly_df.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=weekly_df['week'], y=weekly_df['btts_ece'], \n",
    "                          mode='lines+markers', name='BTTS ECE',\n",
    "                          line=dict(color='orange')),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            fig.add_hline(y=0.07, line_dash=\"dash\", line_color=\"red\", \n",
    "                         annotation_text=\"Threshold\", row=2, col=1)\n",
    "        \n",
    "        # BTTS Brier\n",
    "        if 'btts_brier' in weekly_df.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=weekly_df['week'], y=weekly_df['btts_brier'], \n",
    "                          mode='lines+markers', name='BTTS Brier',\n",
    "                          line=dict(color='purple')),\n",
    "                row=2, col=2\n",
    "            )\n",
    "            fig.add_hline(y=0.20, line_dash=\"dash\", line_color=\"red\", \n",
    "                         annotation_text=\"Threshold\", row=2, col=2)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"ðŸ“ˆ Weekly Calibration Trends\",\n",
    "            height=600,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        print(f\"ðŸ“Š Weekly trends calculated for {len(weekly_df)} weeks\")\n",
    "    else:\n",
    "        print(\"âŒ Not enough data for weekly trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Tuning History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÐŸÐ¾Ð»ÑƒÑ‡Ð°Ð²Ð° tuning Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ\n",
    "tuning_history = tuner.get_tuning_history(days=30)\n",
    "\n",
    "if 'error' not in tuning_history:\n",
    "    print(\"ðŸ”§ TUNING HISTORY (Last 30 days)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    summary = tuning_history['summary']\n",
    "    print(f\"ðŸ“Š Summary:\")\n",
    "    print(f\"   Total actions: {summary['total_actions']}\")\n",
    "    print(f\"   Auto corrections: {summary['auto_corrections']}\")\n",
    "    print(f\"   Manual adjustments: {summary['manual_adjustments']}\")\n",
    "    print()\n",
    "    \n",
    "    if tuning_history['history']:\n",
    "        print(\"ðŸ“‹ Recent Actions:\")\n",
    "        for action in tuning_history['history'][-5:]:  # ÐŸÐ¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ‚Ðµ 5\n",
    "            timestamp = datetime.fromisoformat(action['timestamp']).strftime('%Y-%m-%d %H:%M')\n",
    "            print(f\"   {timestamp}: {action['action']} - {action['reason']}\")\n",
    "            \n",
    "            if 'changes' in action and action['changes']:\n",
    "                for param_group, changes in action['changes'].items():\n",
    "                    if isinstance(changes, dict):\n",
    "                        for param, change_info in changes.items():\n",
    "                            if isinstance(change_info, dict) and 'old' in change_info:\n",
    "                                print(f\"     â†’ {param_group}.{param}: {change_info['old']:.3f} â†’ {change_info['new']:.3f}\")\n",
    "    else:\n",
    "        print(\"âœ… No tuning actions in the last 30 days\")\n",
    "else:\n",
    "    print(f\"âŒ Error getting tuning history: {tuning_history['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Current Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÐŸÐ¾ÐºÐ°Ð·Ð²Ð° Ñ‚ÐµÐºÑƒÑ‰Ð¸Ñ‚Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸\n",
    "current_params = tuner.current_params\n",
    "\n",
    "print(\"ðŸŽ¯ CURRENT MODEL PARAMETERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"ðŸ”§ Soft Clipping:\")\n",
    "print(f\"   Lower bound: {current_params['soft_clipping']['lo']:.3f}\")\n",
    "print(f\"   Upper bound: {current_params['soft_clipping']['hi']:.3f}\")\n",
    "print()\n",
    "\n",
    "print(f\"âš–ï¸ Ensemble Weights:\")\n",
    "for model, weight in current_params['ensemble_weights'].items():\n",
    "    print(f\"   {model.capitalize()}: {weight:.3f}\")\n",
    "print()\n",
    "\n",
    "print(f\"ðŸŽ² Poisson Parameters:\")\n",
    "print(f\"   Shrinkage alpha: {current_params['poisson_params']['shrinkage_alpha']:.3f}\")\n",
    "print(f\"   Home advantage multiplier: {current_params['poisson_params']['home_advantage_multiplier']:.3f}\")\n",
    "print()\n",
    "\n",
    "print(f\"âš½ BTTS Calibration:\")\n",
    "print(f\"   Scaling factor: {current_params['btts_calibration']['scaling_factor']:.3f}\")\n",
    "print(f\"   Poisson blend: {current_params['btts_calibration']['poisson_blend']:.3f}\")\n",
    "print()\n",
    "\n",
    "print(f\"ðŸ¤” Confidence Parameters:\")\n",
    "print(f\"   Entropy weight: {current_params['confidence_params']['entropy_weight']:.3f}\")\n",
    "print(f\"   Agreement weight: {current_params['confidence_params']['agreement_weight']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš¨ Calibration Issues & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð° Ñ‚ÐµÐºÑƒÑ‰Ð¸Ñ‚Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð¸\n",
    "analysis = tuner.analyze_calibration_drift(days=7)\n",
    "\n",
    "if 'error' not in analysis:\n",
    "    print(\"ðŸš¨ CURRENT CALIBRATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"ðŸ“Š Analysis period: {analysis['period']}\")\n",
    "    print(f\"ðŸ“ˆ Matches analyzed: {analysis['n_matches']}\")\n",
    "    print(f\"âš ï¸ Issues detected: {len(analysis['issues_detected'])}\")\n",
    "    print()\n",
    "    \n",
    "    if analysis['issues_detected']:\n",
    "        print(\"ðŸš¨ ISSUES FOUND:\")\n",
    "        for i, issue in enumerate(analysis['issues_detected'], 1):\n",
    "            print(f\"   {i}. {issue['market'].upper()}: {issue['issue'].replace('_', ' ').title()}\")\n",
    "            print(f\"      Value: {issue['value']:.4f} (threshold: {issue['threshold']:.4f})\")\n",
    "            \n",
    "            # ÐŸÐ¾ÐºÐ°Ð·Ð²Ð° severity\n",
    "            severity_ratio = issue['value'] / issue['threshold']\n",
    "            if severity_ratio > 1.5:\n",
    "                severity = \"ðŸ”´ Critical\"\n",
    "            elif severity_ratio > 1.2:\n",
    "                severity = \"ðŸŸ¡ Moderate\"\n",
    "            else:\n",
    "                severity = \"ðŸŸ¢ Minor\"\n",
    "            print(f\"      Severity: {severity} ({severity_ratio:.2f}x threshold)\")\n",
    "            print()\n",
    "        \n",
    "        print(\"ðŸ’¡ RECOMMENDATIONS:\")\n",
    "        for i, rec in enumerate(analysis['recommendations'], 1):\n",
    "            print(f\"   {i}. {rec['description']}\")\n",
    "            print(f\"      Action: {rec['action']}\")\n",
    "            print(f\"      Market: {rec['market']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"âœ… No calibration issues detected!\")\n",
    "        print(\"   The model is well-calibrated within acceptable thresholds.\")\n",
    "else:\n",
    "    print(f\"âŒ Error in calibration analysis: {analysis['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“‹ CALIBRATION DASHBOARD SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(df_with_results) > 0:\n",
    "    print(f\"ðŸ“Š Data Status: âœ… {len(df_with_results)} matches with results\")\n",
    "    \n",
    "    if 'error' not in report:\n",
    "        # ÐžÑ†ÐµÐ½ÑÐ²Ð° Ð¾Ð±Ñ‰Ð¾Ñ‚Ð¾ ÑÑŠÑÑ‚Ð¾ÑÐ½Ð¸Ðµ\n",
    "        issues_count = 0\n",
    "        if 'ou25' in report and report['ou25']['ece'] > 0.07:\n",
    "            issues_count += 1\n",
    "        if 'btts' in report and report['btts']['ece'] > 0.07:\n",
    "            issues_count += 1\n",
    "        if '1x2' in report and 'overall' in report['1x2'] and report['1x2']['overall']['brier_score'] > 0.20:\n",
    "            issues_count += 1\n",
    "        \n",
    "        if issues_count == 0:\n",
    "            status = \"ðŸŸ¢ EXCELLENT\"\n",
    "        elif issues_count <= 1:\n",
    "            status = \"ðŸŸ¡ GOOD\"\n",
    "        else:\n",
    "            status = \"ðŸ”´ NEEDS ATTENTION\"\n",
    "        \n",
    "        print(f\"ðŸŽ¯ Calibration Status: {status}\")\n",
    "        print(f\"âš ï¸ Issues detected: {issues_count}\")\n",
    "    else:\n",
    "        print(f\"ðŸ“Š Data Status: âŒ Error generating calibration report\")\n",
    "else:\n",
    "    print(f\"ðŸ“Š Data Status: âŒ No prediction data available\")\n",
    "\n",
    "print(f\"ðŸ”§ Tuning Status: {summary['total_actions']} actions in last 30 days\")\n",
    "print(f\"ðŸ“… Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nðŸš€ NEXT STEPS:\")\n",
    "if len(df_with_results) == 0:\n",
    "    print(\"   1. Ensure predictions are being logged properly\")\n",
    "    print(\"   2. Add match results to enable calibration analysis\")\n",
    "elif 'error' in analysis or analysis['n_matches'] < 100:\n",
    "    print(\"   1. Collect more prediction data (need 100+ matches)\")\n",
    "    print(\"   2. Ensure match results are being updated\")\n",
    "elif analysis['issues_detected']:\n",
    "    print(\"   1. Consider applying recommended corrections\")\n",
    "    print(\"   2. Monitor calibration trends closely\")\n",
    "    print(\"   3. Run daily monitoring script\")\n",
    "else:\n",
    "    print(\"   1. Continue regular monitoring\")\n",
    "    print(\"   2. Maintain current parameter settings\")\n",
    "    print(\"   3. Review weekly calibration reports\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Dashboard analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
